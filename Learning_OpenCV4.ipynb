{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Learning OpenCV4 ",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMVkpHqeHNn0F7s21HraA3S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mawhy/OpenCV/blob/master/Learning_OpenCV4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08QCDaFB2ZcD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab.patches import cv2_imshow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fZ2qz7q2lTZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/PacktPublishing/Learning-OpenCV-4-Computer-Vision-with-Python-Third-Edition.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KteqBObf20jn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cp \"/content/Learning-OpenCV-4-Computer-Vision-with-Python-Third-Edition/chapter02/MyPic.png\" \"/content/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnuHOMjR3fWP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "\n",
        "image = cv2.imread('MyPic.png')\n",
        "cv2.imwrite('MyPic.jpg', image)\n",
        "img = cv2.imread('MyPic.jpg', cv2.IMREAD_UNCHANGED)\n",
        "cv2_imshow(img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzG2n1QY3pg-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "\n",
        "grayImage = cv2.imread('MyPic.png', cv2.IMREAD_GRAYSCALE)\n",
        "cv2.imwrite('MyPicGray.png', grayImage)\n",
        "img = cv2.imread('MyPicGray.png', cv2.IMREAD_UNCHANGED)\n",
        "cv2_imshow(img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDSWbMX24qal",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cp \"/content/Learning-OpenCV-4-Computer-Vision-with-Python-Third-Edition/chapter02/MyInputVid.avi\" \"/content/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JY-HaL6n4og3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "\n",
        "videoCapture = cv2.VideoCapture('MyInputVid.avi')\n",
        "fps = videoCapture.get(cv2.CAP_PROP_FPS)\n",
        "size = (int(videoCapture.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
        "        int(videoCapture.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
        "videoWriter = cv2.VideoWriter(\n",
        "    'MyOutputVid.avi', cv2.VideoWriter_fourcc('I','4','2','0'), fps, size)\n",
        "\n",
        "success, frame = videoCapture.read()\n",
        "while success: # Loop until there are no more frames.\n",
        "    videoWriter.write(frame)\n",
        "    success, frame = videoCapture.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQAfXBNn8P6e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# camera = cv2.VideoCapture(0)\n",
        "frame = cv2.imread('MyPic.png')\n",
        "# determine upper and lower HSV limits for (my) skin tones\n",
        "lower = np.array([0, 100, 0], dtype=\"uint8\")\n",
        "upper = np.array([50,255,255], dtype=\"uint8\")\n",
        "\n",
        "    #ret, frame = camera.read()\n",
        "    #if not ret:\n",
        "    #    continue\n",
        "    # switch to HSV\n",
        "hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
        "    # find mask of pixels within HSV range\n",
        "skinMask = cv2.inRange(hsv, lower, upper)\n",
        "    # denoise\n",
        "skinMask = cv2.GaussianBlur(skinMask, (9, 9), 0)\n",
        "    # kernel for morphology operation\n",
        "kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (4, 4))\n",
        "    # CLOSE (dilate / erode)\n",
        "skinMask = cv2.morphologyEx(skinMask, cv2.MORPH_CLOSE, kernel, iterations = 3)\n",
        "    # denoise the mask\n",
        "skinMask = cv2.GaussianBlur(skinMask, (9, 9), 0)\n",
        "    # only display the masked pixels\n",
        "skin = cv2.bitwise_and(frame, frame, mask = skinMask)\n",
        "cv2_imshow(skin)\n",
        "    # quit or save frame\n",
        "cv2.imwrite(\"skin.jpg\", skin) \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fjo8zH9oOcag",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cp -av \"/content/Learning-OpenCV-4-Computer-Vision-with-Python-Third-Edition/images\" \"/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2XQ82NC9FM0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "img = cv2.imread(\"../images/statue_small.jpg\", 0)\n",
        "cv2.imwrite(\"canny.jpg\", cv2.Canny(img, 200, 300))\n",
        "cv2_imshow(cv2.imread(\"canny.jpg\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "En17EpYAOts5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "OPENCV_MAJOR_VERSION = int(cv2.__version__.split('.')[0])\n",
        "\n",
        "img = np.zeros((200, 200), dtype=np.uint8)\n",
        "img[50:150, 50:150] = 255\n",
        "\n",
        "ret, thresh = cv2.threshold(img, 127, 255, 0)\n",
        "\n",
        "if OPENCV_MAJOR_VERSION >= 4:\n",
        "    # OpenCV 4 or a later version is being used.\n",
        "    contours, hier = cv2.findContours(thresh, cv2.RETR_TREE,\n",
        "                                      cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "color = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
        "img = cv2.drawContours(color, contours, -1, (0,255,0), 2)\n",
        "cv2_imshow(color)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FBasbNhPiel",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cp -av \"/content/Learning-OpenCV-4-Computer-Vision-with-Python-Third-Edition/chapter03/hammer.jpg\" \"/content/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M76L_eUWPLyT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "OPENCV_MAJOR_VERSION = int(cv2.__version__.split('.')[0])\n",
        "\n",
        "img = cv2.pyrDown(cv2.imread(\"hammer.jpg\", cv2.IMREAD_UNCHANGED))\n",
        "\n",
        "ret, thresh = cv2.threshold(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY),\n",
        "                            127, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "if OPENCV_MAJOR_VERSION >= 4:\n",
        "    # OpenCV 4 or a later version is being used.\n",
        "    contours, hier = cv2.findContours(thresh, cv2.RETR_EXTERNAL,\n",
        "                                      cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "for c in contours:\n",
        "    # find bounding box coordinates\n",
        "    x, y, w, h = cv2.boundingRect(c)\n",
        "    cv2.rectangle(img, (x,y), (x+w, y+h), (0, 255, 0), 2)\n",
        "\n",
        "    # find minimum area\n",
        "    rect = cv2.minAreaRect(c)\n",
        "    # calculate coordinates of the minimum area rectangle\n",
        "    box = cv2.boxPoints(rect)\n",
        "    # normalize coordinates to integers\n",
        "    box = np.int0(box)\n",
        "    # draw contours\n",
        "    cv2.drawContours(img, [box], 0, (0,0, 255), 3)\n",
        "\n",
        "    # calculate center and radius of minimum enclosing circle\n",
        "    (x, y), radius = cv2.minEnclosingCircle(c)\n",
        "    # cast to integers\n",
        "    center = (int(x), int(y))\n",
        "    radius = int(radius)\n",
        "    # draw the circle\n",
        "    img = cv2.circle(img, center, radius, (0, 255, 0), 2)\n",
        "\n",
        "cv2.drawContours(img, contours, -1, (255, 0, 0), 1)\n",
        "cv2_imshow(img)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiWZf4d0P3nl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "OPENCV_MAJOR_VERSION = int(cv2.__version__.split('.')[0])\n",
        "\n",
        "img = cv2.pyrDown(cv2.imread(\"hammer.jpg\", cv2.IMREAD_UNCHANGED))\n",
        "\n",
        "ret, thresh = cv2.threshold(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY),\n",
        "                            127, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "if OPENCV_MAJOR_VERSION >= 4:\n",
        "    # OpenCV 4 or a later version is being used.\n",
        "    contours, hier = cv2.findContours(thresh, cv2.RETR_EXTERNAL,\n",
        "                                      cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "black = np.zeros_like(img)\n",
        "for cnt in contours:\n",
        "    epsilon = 0.01 * cv2.arcLength(cnt,True)\n",
        "    approx = cv2.approxPolyDP(cnt,epsilon,True)\n",
        "    hull = cv2.convexHull(cnt)\n",
        "    cv2.drawContours(black, [cnt], -1, (0, 255, 0), 2)\n",
        "    cv2.drawContours(black, [approx], -1, (255, 255, 0), 2)\n",
        "    cv2.drawContours(black, [hull], -1, (0, 0, 255), 2)\n",
        "\n",
        "cv2_imshow(black)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iakqlpJERGkW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cp -av \"/content/Learning-OpenCV-4-Computer-Vision-with-Python-Third-Edition/chapter03/planet_glow.jpg\" \"/content/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzIBG_WHQQ34",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "planets = cv2.imread('planet_glow.jpg')\n",
        "gray_img = cv2.cvtColor(planets, cv2.COLOR_BGR2GRAY)\n",
        "gray_img = cv2.medianBlur(gray_img, 5)\n",
        "\n",
        "circles = cv2.HoughCircles(gray_img,cv2.HOUGH_GRADIENT,1,120,\n",
        "                           param1=100,param2=30,minRadius=0,maxRadius=0)\n",
        "\n",
        "circles = np.uint16(np.around(circles))\n",
        "\n",
        "for i in circles[0,:]:\n",
        "    # draw the outer circle\n",
        "    cv2.circle(planets,(i[0],i[1]),i[2],(0,255,0),2)\n",
        "    # draw the center of the circle\n",
        "    cv2.circle(planets,(i[0],i[1]),2,(0,0,255),3)\n",
        "\n",
        "cv2.imwrite(\"planets_circles.jpg\", planets)\n",
        "cv2_imshow(planets)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCy60DSpRVvF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plot\n",
        "\n",
        "img = cv2.imread('../images/bb.jpg', 0)\n",
        "f = np.fft.fft2(img)\n",
        "fshift = np.fft.fftshift(f)\n",
        "magnitude_spectrum = 20 * np.log(np.abs(fshift))\n",
        "\n",
        "row, cols = img.shape\n",
        "crow, ccol = row // 2, cols // 2\n",
        "fshift[crow - 30: crow+30, ccol - 30: ccol + 30] = 0\n",
        "\n",
        "f_ishift = np.fft.ifftshift(fshift)\n",
        "img_back = np.fft.ifft2(f_ishift)\n",
        "img_back = np.abs(img_back)\n",
        "\n",
        "plot.subplot(221), plot.imshow(img, cmap = \"gray\")\n",
        "plot.title(\"Input\"), plot.xticks([]), plot.yticks([])\n",
        "\n",
        "plot.subplot(222), plot.imshow(magnitude_spectrum, cmap = \"gray\")\n",
        "plot.title('magnitude_spectrum'), plot.xticks([]), plot.yticks([])\n",
        "\n",
        "plot.subplot(223), plot.imshow(img_back, cmap = \"gray\")\n",
        "plot.title(\"Input in JET\"), plot.xticks([]), plot.yticks([])\n",
        "plot.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HDlzzVfSlE1",
        "colab_type": "text"
      },
      "source": [
        "### HPF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhfvKijoRti3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from scipy import ndimage\n",
        "\n",
        "kernel_3x3 = np.array([[-1, -1, -1],\n",
        "                       [-1,  8, -1],\n",
        "                       [-1, -1, -1]])\n",
        "\n",
        "kernel_5x5 = np.array([[-1, -1, -1, -1, -1],\n",
        "                       [-1,  1,  2,  1, -1],\n",
        "                       [-1,  2,  4,  2, -1],\n",
        "                       [-1,  1,  2,  1, -1],\n",
        "                       [-1, -1, -1, -1, -1]])\n",
        "\n",
        "img = cv2.imread(\"../images/statue_small.jpg\", 0)\n",
        "\n",
        "k3 = ndimage.convolve(img, kernel_3x3)\n",
        "k5 = ndimage.convolve(img, kernel_5x5)\n",
        "\n",
        "blurred = cv2.GaussianBlur(img, (17,17), 0)\n",
        "g_hpf = img - blurred\n",
        "\n",
        "cv2_imshow(k3)\n",
        "cv2_imshow(k5)\n",
        "cv2_imshow(blurred)\n",
        "cv2_imshow(g_hpf)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lLx1AeLS5AK",
        "colab_type": "text"
      },
      "source": [
        "### Grabcut"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FbN8ZuCSSXP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "original = cv2.imread('../images/statue_small.jpg')\n",
        "img = original.copy()\n",
        "mask = np.zeros(img.shape[:2], np.uint8)\n",
        "\n",
        "bgdModel = np.zeros((1, 65), np.float64)\n",
        "fgdModel = np.zeros((1, 65), np.float64)\n",
        "\n",
        "rect = (100, 1, 421, 378)\n",
        "cv2.grabCut(img, mask, rect, bgdModel, fgdModel, 5, cv2.GC_INIT_WITH_RECT)\n",
        "\n",
        "mask2 = np.where((mask==2) | (mask==0), 0, 1).astype('uint8')\n",
        "img = img*mask2[:,:,np.newaxis]\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "plt.title(\"grabcut\")\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.imshow(cv2.cvtColor(original, cv2.COLOR_BGR2RGB))\n",
        "plt.title(\"original\")\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYdGM4CUTS87",
        "colab_type": "text"
      },
      "source": [
        "### Watershed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTGGthGYTBYZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "img = cv2.imread('../images/5_of_diamonds.png')\n",
        "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "ret, thresh = cv2.threshold(gray, 0, 255,\n",
        "                            cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)\n",
        "\n",
        "# Remove noise.\n",
        "kernel = np.ones((3,3), np.uint8)\n",
        "opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel,\n",
        "                           iterations = 2)\n",
        "\n",
        "# Find the sure background region.\n",
        "sure_bg = cv2.dilate(opening, kernel, iterations = 3)\n",
        "\n",
        "# Find the sure foreground region.\n",
        "dist_transform = cv2.distanceTransform(opening,cv2.DIST_L2,5)\n",
        "ret, sure_fg = cv2.threshold(\n",
        "        dist_transform, 0.7*dist_transform.max(), 255, 0)\n",
        "sure_fg = sure_fg.astype(np.uint8)\n",
        "\n",
        "# Find the unknown region.\n",
        "unknown = cv2.subtract(sure_bg, sure_fg)\n",
        "\n",
        "# Label the foreground objects.\n",
        "ret, markers = cv2.connectedComponents(sure_fg)\n",
        "\n",
        "# Add one to all labels so that sure background is not 0, but 1.\n",
        "markers += 1\n",
        "\n",
        "# Label the unknown region as 0.\n",
        "markers[unknown==255] = 0\n",
        "\n",
        "markers = cv2.watershed(img, markers)\n",
        "img[markers==-1] = [255,0,0]\n",
        "\n",
        "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwWm7_GWdh_O",
        "colab_type": "text"
      },
      "source": [
        "### Face Recognition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61MGuz5ZcGXm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cp -av \"/content/Learning-OpenCV-4-Computer-Vision-with-Python-Third-Edition/chapter05/cascades/haarcascade_frontalface_default.xml\" \"/content/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9ArMHh1T-0a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "\n",
        "\n",
        "face_cascade = cv2.CascadeClassifier('/content/haarcascade_frontalface_default.xml')\n",
        "img = cv2.imread('../images/woodcutters.jpg')\n",
        "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "faces = face_cascade.detectMultiScale(gray, 1.08, 5)\n",
        "for (x, y, w, h) in faces:\n",
        "    img = cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
        "  \n",
        "cv2_imshow(img)\n",
        "cv2.imwrite('./woodcutters_detected.jpg', img)\n",
        "cv2.waitKey(0)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYsbwf1TfHsX",
        "colab_type": "text"
      },
      "source": [
        "### SIFT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAePKuLlemds",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip uninstall opencv-python -y\n",
        "# downgrade OpenCV a bit since some none-free features are not avilable\n",
        "!pip install opencv-contrib-python==3.4.2.17 --force-reinstall"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7U3WFi5BcmIB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "img = cv2.imread('../images/varese.jpg')\n",
        "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "sift = cv2.xfeatures2d.SIFT_create()\n",
        "keypoints, descriptors = sift.detectAndCompute(gray, None)\n",
        "\n",
        "cv2.drawKeypoints(img, keypoints, img, (51, 163, 236),\n",
        "                  cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
        "\n",
        "cv2_imshow(img)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SSkKj4Bfmlk",
        "colab_type": "text"
      },
      "source": [
        "### SURF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEdiYeGoeHRg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "\n",
        "img = cv2.imread('../images/varese.jpg')\n",
        "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "surf = cv2.xfeatures2d.SURF_create(8000)\n",
        "keypoints, descriptors = surf.detectAndCompute(gray, None)\n",
        "\n",
        "cv2.drawKeypoints(img, keypoints, img, (51, 163, 236),\n",
        "                  cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
        "\n",
        "cv2_imshow(img)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljt0Ht13f36x",
        "colab_type": "text"
      },
      "source": [
        "### CornerHarris"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z99D2HZjfbLW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "\n",
        "img = cv2.imread('../images/chess_board.png')\n",
        "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "dst = cv2.cornerHarris(gray, 2, 23, 0.04)\n",
        "img[dst > 0.01 * dst.max()] = [0, 0, 255]\n",
        "cv2_imshow(img)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P90paBPdgIwd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUWU95RcgLWQ",
        "colab_type": "text"
      },
      "source": [
        "### FLANN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCWbzaP3f1b2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "img0 = cv2.imread('../images/gauguin_entre_les_lys.jpg',\n",
        "                  cv2.IMREAD_GRAYSCALE)\n",
        "img1 = cv2.imread('../images/gauguin_paintings.png',\n",
        "                  cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "# Perform SIFT feature detection and description.\n",
        "sift = cv2.xfeatures2d.SIFT_create()\n",
        "kp0, des0 = sift.detectAndCompute(img0, None)\n",
        "kp1, des1 = sift.detectAndCompute(img1, None)\n",
        "\n",
        "# Define FLANN-based matching parameters.\n",
        "FLANN_INDEX_KDTREE = 1\n",
        "index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
        "search_params = dict(checks=50)\n",
        "\n",
        "# Perform FLANN-based matching.\n",
        "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
        "matches = flann.knnMatch(des0, des1, k=2)\n",
        "\n",
        "# Prepare an empty mask to draw good matches.\n",
        "mask_matches = [[0, 0] for i in range(len(matches))]\n",
        "\n",
        "# Populate the mask based on David G. Lowe's ratio test.\n",
        "for i, (m, n) in enumerate(matches):\n",
        "    if m.distance < 0.7 * n.distance:\n",
        "        mask_matches[i]=[1, 0]\n",
        "\n",
        "# Draw the matches that passed the ratio test.\n",
        "img_matches = cv2.drawMatchesKnn(\n",
        "    img0, kp0, img1, kp1, matches, None,\n",
        "    matchColor=(0, 255, 0), singlePointColor=(255, 0, 0),\n",
        "    matchesMask=mask_matches, flags=0)\n",
        "\n",
        "# Show the matches.\n",
        "plt.imshow(img_matches)\n",
        "plt.show()\n",
        "cv2_imshow(img_matches)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2-PVteTh6gs",
        "colab_type": "text"
      },
      "source": [
        "### Create descriptors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJUfqdmyhDBa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cp -av \"/content/Learning-OpenCV-4-Computer-Vision-with-Python-Third-Edition/chapter06/tattoos\" \"/content/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "it3ZbYutgPW5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "def create_descriptors(folder):\n",
        "    feature_detector = cv2.xfeatures2d.SIFT_create()\n",
        "    files = []\n",
        "    for (dirpath, dirnames, filenames) in os.walk(folder):\n",
        "        files.extend(filenames)\n",
        "    for f in files:\n",
        "        create_descriptor(folder, f, feature_detector)\n",
        "\n",
        "def create_descriptor(folder, image_path, feature_detector):\n",
        "    if not image_path.endswith('png'):\n",
        "        print('skipping %s' % image_path)\n",
        "        return\n",
        "    print('reading %s' % image_path)\n",
        "    img = cv2.imread(os.path.join(folder, image_path),\n",
        "                     cv2.IMREAD_GRAYSCALE)\n",
        "    keypoints, descriptors = feature_detector.detectAndCompute(\n",
        "        img, None)\n",
        "    descriptor_file = image_path.replace('png', 'npy')\n",
        "    np.save(os.path.join(folder, descriptor_file), descriptors)\n",
        "\n",
        "folder = 'tattoos'\n",
        "create_descriptors(folder)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B64n4NlTiax6",
        "colab_type": "text"
      },
      "source": [
        "### Homography"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cq4jIfbQiVuB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "MIN_NUM_GOOD_MATCHES = 10\n",
        "\n",
        "img0 = cv2.imread('tattoos/query.png',\n",
        "                  cv2.IMREAD_GRAYSCALE)\n",
        "img1 = cv2.imread('tattoos/anchor-man.png',\n",
        "                  cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "# Perform SIFT feature detection and description.\n",
        "sift = cv2.xfeatures2d.SIFT_create()\n",
        "kp0, des0 = sift.detectAndCompute(img0, None)\n",
        "kp1, des1 = sift.detectAndCompute(img1, None)\n",
        "\n",
        "# Define FLANN-based matching parameters.\n",
        "FLANN_INDEX_KDTREE = 1\n",
        "index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
        "search_params = dict(checks=50)\n",
        "\n",
        "# Perform FLANN-based matching.\n",
        "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
        "matches = flann.knnMatch(des0, des1, k=2)\n",
        "\n",
        "# Find all the good matches as per Lowe's ratio test.\n",
        "good_matches = []\n",
        "for m, n in matches:\n",
        "    if m.distance < 0.7 * n.distance:\n",
        "        good_matches.append(m)\n",
        "\n",
        "if len(good_matches) >= MIN_NUM_GOOD_MATCHES:\n",
        "    src_pts = np.float32(\n",
        "        [kp0[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
        "    dst_pts = np.float32(\n",
        "        [kp1[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
        "\n",
        "    M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
        "    mask_matches = mask.ravel().tolist()\n",
        "\n",
        "    h, w = img0.shape\n",
        "    src_corners = np.float32(\n",
        "        [[0, 0], [0, h-1], [w-1, h-1], [w-1, 0]]).reshape(-1, 1, 2)\n",
        "    dst_corners = cv2.perspectiveTransform(src_corners, M)\n",
        "    dst_corners = dst_corners.astype(np.int32)\n",
        "\n",
        "    # Draw the bounds of the matched region based on the homography.\n",
        "    num_corners = len(dst_corners)\n",
        "    for i in range(num_corners):\n",
        "        x0, y0 = dst_corners[i][0]\n",
        "        if i == num_corners - 1:\n",
        "            next_i = 0\n",
        "        else:\n",
        "            next_i = i + 1\n",
        "        x1, y1 = dst_corners[next_i][0]\n",
        "        cv2.line(img1, (x0, y0), (x1, y1), 255, 3, cv2.LINE_AA)\n",
        "\n",
        "    # Draw the matches that passed the ratio test.\n",
        "    img_matches = cv2.drawMatches(\n",
        "        img0, kp0, img1, kp1, good_matches, None,\n",
        "        matchColor=(0, 255, 0), singlePointColor=None,\n",
        "        matchesMask=mask_matches, flags=2)\n",
        "\n",
        "    # Show the homography and good matches.\n",
        "    plt.imshow(img_matches)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Not enough matches good were found - %d/%d\" % \\\n",
        "          (len(good_matches), MIN_MATCH_COUNT))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_Y0lPWriuKw",
        "colab_type": "text"
      },
      "source": [
        "### ORB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZX29p19ig76",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Load the images.\n",
        "img0 = cv2.imread('../images/nasa_logo.png',\n",
        "                  cv2.IMREAD_GRAYSCALE)\n",
        "img1 = cv2.imread('../images/kennedy_space_center.jpg',\n",
        "                  cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "# Perform ORB feature detection and description.\n",
        "orb = cv2.ORB_create()\n",
        "kp0, des0 = orb.detectAndCompute(img0, None)\n",
        "kp1, des1 = orb.detectAndCompute(img1, None)\n",
        "\n",
        "# Perform brute-force matching.\n",
        "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
        "matches = bf.match(des0, des1)\n",
        "\n",
        "# Sort the matches by distance.\n",
        "matches = sorted(matches, key=lambda x:x.distance)\n",
        "\n",
        "# Draw the best 25 matches.\n",
        "img_matches = cv2.drawMatches(\n",
        "    img0, kp0, img1, kp1, matches[:25], img1,\n",
        "    flags=cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n",
        "\n",
        "# Show the matches.\n",
        "plt.imshow(img_matches)\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8BB5ax2jAfx",
        "colab_type": "text"
      },
      "source": [
        "### ORB with KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ykc0kt5Piwou",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Load the images.\n",
        "img0 = cv2.imread('../images/nasa_logo.png',\n",
        "                  cv2.IMREAD_GRAYSCALE)\n",
        "img1 = cv2.imread('../images/kennedy_space_center.jpg',\n",
        "                  cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "# Perform ORB feature detection and description.\n",
        "orb = cv2.ORB_create()\n",
        "kp0, des0 = orb.detectAndCompute(img0, None)\n",
        "kp1, des1 = orb.detectAndCompute(img1, None)\n",
        "\n",
        "# Perform brute-force KNN matching.\n",
        "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=False)\n",
        "pairs_of_matches = bf.knnMatch(des0, des1, k=2)\n",
        "\n",
        "# Sort the pairs of matches by distance.\n",
        "pairs_of_matches = sorted(pairs_of_matches, key=lambda x:x[0].distance)\n",
        "\n",
        "# Draw the 25 best pairs of matches.\n",
        "img_pairs_of_matches = cv2.drawMatchesKnn(\n",
        "    img0, kp0, img1, kp1, pairs_of_matches[:25], img1,\n",
        "    flags=cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n",
        "\n",
        "# Show the pairs of matches.\n",
        "plt.imshow(img_pairs_of_matches)\n",
        "plt.show()\n",
        "\n",
        "# Apply the ratio test.\n",
        "matches = [x[0] for x in pairs_of_matches\n",
        "           if len(x) > 1 and x[0].distance < 0.8 * x[1].distance]\n",
        "\n",
        "# Draw the best 25 matches.\n",
        "img_matches = cv2.drawMatches(\n",
        "    img0, kp0, img1, kp1, matches[:25], img1,\n",
        "    flags=cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n",
        "\n",
        "# Show the matches.\n",
        "plt.imshow(img_matches)\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKdkFUUpjWMO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SCR1PwejXWO",
        "colab_type": "text"
      },
      "source": [
        "### Scan matches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lT9c29gSjEbO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# Read the query image.\n",
        "folder = 'tattoos'\n",
        "query = cv2.imread(os.path.join(folder, 'query.png'),\n",
        "                   cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "# create files, images, descriptors globals\n",
        "files = []\n",
        "images = []\n",
        "descriptors = []\n",
        "for (dirpath, dirnames, filenames) in os.walk(folder):\n",
        "    files.extend(filenames)\n",
        "    for f in files:\n",
        "        if f.endswith('npy') and f != 'query.npy':\n",
        "            descriptors.append(f)\n",
        "print(descriptors)\n",
        "\n",
        "# Create the SIFT detector.\n",
        "sift = cv2.xfeatures2d.SIFT_create()\n",
        "\n",
        "# Perform SIFT feature detection and description on the\n",
        "# query image.\n",
        "query_kp, query_ds = sift.detectAndCompute(query, None)\n",
        "\n",
        "# Define FLANN-based matching parameters.\n",
        "FLANN_INDEX_KDTREE = 1\n",
        "index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
        "search_params = dict(checks=50)\n",
        "\n",
        "# Create the FLANN matcher.\n",
        "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
        "\n",
        "# Define the minimum number of good matches for a suspect.\n",
        "MIN_NUM_GOOD_MATCHES = 10\n",
        "\n",
        "greatest_num_good_matches = 0\n",
        "prime_suspect = None\n",
        "\n",
        "print('>> Initiating picture scan...')\n",
        "for d in descriptors:\n",
        "    print('--------- analyzing %s for matches ------------' % d)\n",
        "    matches = flann.knnMatch(\n",
        "        query_ds, np.load(os.path.join(folder, d)), k=2)\n",
        "    good_matches = []\n",
        "    for m, n in matches:\n",
        "        if m.distance < 0.7 * n.distance:\n",
        "            good_matches.append(m)\n",
        "    num_good_matches = len(good_matches)\n",
        "    name = d.replace('.npy', '').upper()\n",
        "    if num_good_matches >= MIN_NUM_GOOD_MATCHES:\n",
        "        print('%s is a suspect! (%d matches)' % \\\n",
        "            (name, num_good_matches))\n",
        "        if num_good_matches > greatest_num_good_matches:\n",
        "            greatest_num_good_matches = num_good_matches\n",
        "            prime_suspect = name\n",
        "    else:\n",
        "        print('%s is NOT a suspect. (%d matches)' % \\\n",
        "            (name, num_good_matches))\n",
        "\n",
        "if prime_suspect is not None:\n",
        "    print('Prime suspect is %s.' % prime_suspect)\n",
        "else:\n",
        "    print('There is no suspect.')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HGjVC1r26OC",
        "colab_type": "text"
      },
      "source": [
        "### Detect Car with SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_P1EGc5kAye",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://github.com/Menuka5/Data-sets-for-opencv-classifier-training/blob/master/Other%20Image%20Datasets%20collected/UIUC%20Image%20Database%20for%20Car%20Detection/CarData.zip?raw=true\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UH5xcf9SltaR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip \"/content/CarData.zip?raw=true\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eO3OaJ5e0nwI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%rm -rf \"/content/carData\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6mLX4VIjcQD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "if not os.path.isdir('CarData'):\n",
        "    print('CarData folder not found. Please download and unzip '\n",
        "          'http://l2r.cs.uiuc.edu/~cogcomp/Data/Car/CarData.tar.gz '\n",
        "          'into the same folder as this script.')\n",
        "    exit(1)\n",
        "\n",
        "BOW_NUM_TRAINING_SAMPLES_PER_CLASS = 10\n",
        "SVM_NUM_TRAINING_SAMPLES_PER_CLASS = 100\n",
        "\n",
        "sift = cv2.xfeatures2d.SIFT_create()\n",
        "\n",
        "FLANN_INDEX_KDTREE = 1\n",
        "index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
        "search_params = {}\n",
        "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
        "\n",
        "bow_kmeans_trainer = cv2.BOWKMeansTrainer(40)\n",
        "bow_extractor = cv2.BOWImgDescriptorExtractor(sift, flann)\n",
        "\n",
        "def get_pos_and_neg_paths(i):\n",
        "    pos_path = 'CarData/TrainImages/pos-%d.pgm' % (i+1)\n",
        "    neg_path = 'CarData/TrainImages/neg-%d.pgm' % (i+1)\n",
        "    return pos_path, neg_path\n",
        "\n",
        "def add_sample(path):\n",
        "    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
        "    keypoints, descriptors = sift.detectAndCompute(img, None)\n",
        "    if descriptors is not None:\n",
        "        bow_kmeans_trainer.add(descriptors)\n",
        "\n",
        "for i in range(BOW_NUM_TRAINING_SAMPLES_PER_CLASS):\n",
        "    pos_path, neg_path = get_pos_and_neg_paths(i)\n",
        "    add_sample(pos_path)\n",
        "    add_sample(neg_path)\n",
        "\n",
        "voc = bow_kmeans_trainer.cluster()\n",
        "bow_extractor.setVocabulary(voc)\n",
        "\n",
        "def extract_bow_descriptors(img):\n",
        "    features = sift.detect(img)\n",
        "    return bow_extractor.compute(img, features)\n",
        "\n",
        "training_data = []\n",
        "training_labels = []\n",
        "for i in range(SVM_NUM_TRAINING_SAMPLES_PER_CLASS):\n",
        "    pos_path, neg_path = get_pos_and_neg_paths(i)\n",
        "    pos_img = cv2.imread(pos_path, cv2.IMREAD_GRAYSCALE)\n",
        "    pos_descriptors = extract_bow_descriptors(pos_img)\n",
        "    if pos_descriptors is not None:\n",
        "        training_data.extend(pos_descriptors)\n",
        "        training_labels.append(1)\n",
        "    neg_img = cv2.imread(neg_path, cv2.IMREAD_GRAYSCALE)\n",
        "    neg_descriptors = extract_bow_descriptors(neg_img)\n",
        "    if neg_descriptors is not None:\n",
        "        training_data.extend(neg_descriptors)\n",
        "        training_labels.append(-1)\n",
        "\n",
        "svm = cv2.ml.SVM_create()\n",
        "svm.train(np.array(training_data), cv2.ml.ROW_SAMPLE,\n",
        "          np.array(training_labels))\n",
        "\n",
        "for test_img_path in ['CarData/TestImages/test-0.pgm',\n",
        "                      'CarData/TestImages/test-1.pgm',\n",
        "                      '../images/car.jpg',\n",
        "                      '../images/haying.jpg',\n",
        "                      '../images/statue.jpg',\n",
        "                      '../images/woodcutters.jpg']:\n",
        "    img = cv2.imread(test_img_path)\n",
        "    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    descriptors = extract_bow_descriptors(gray_img)\n",
        "    prediction = svm.predict(descriptors)\n",
        "    if prediction[1][0][0] == 1.0:\n",
        "        text = 'car'\n",
        "        color = (0, 255, 0)\n",
        "    else:\n",
        "        text = 'not car'\n",
        "        color = (0, 0, 255)\n",
        "    cv2.putText(img, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
        "                color, 2, cv2.LINE_AA)\n",
        "    cv2_imshow(img)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0-zCIhp3_r7",
        "colab_type": "text"
      },
      "source": [
        "### Slinding Windows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVE4IJvgkC5j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cp -av \"/content/Learning-OpenCV-4-Computer-Vision-with-Python-Third-Edition/chapter07/non_max_suppression.py\" \"/content/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Th2knlvZ3thZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from non_max_suppression import non_max_suppression_fast as nms\n",
        "\n",
        "if not os.path.isdir('CarData'):\n",
        "    print('CarData folder not found. Please download and unzip '\n",
        "          'http://l2r.cs.uiuc.edu/~cogcomp/Data/Car/CarData.tar.gz '\n",
        "          'into the same folder as this script.')\n",
        "    exit(1)\n",
        "\n",
        "BOW_NUM_TRAINING_SAMPLES_PER_CLASS = 10\n",
        "SVM_NUM_TRAINING_SAMPLES_PER_CLASS = 100\n",
        "\n",
        "SVM_SCORE_THRESHOLD = 1.8\n",
        "NMS_OVERLAP_THRESHOLD = 0.15\n",
        "\n",
        "sift = cv2.xfeatures2d.SIFT_create()\n",
        "\n",
        "FLANN_INDEX_KDTREE = 1\n",
        "index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
        "search_params = {}\n",
        "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
        "\n",
        "bow_kmeans_trainer = cv2.BOWKMeansTrainer(12)\n",
        "bow_extractor = cv2.BOWImgDescriptorExtractor(sift, flann)\n",
        "\n",
        "def get_pos_and_neg_paths(i):\n",
        "    pos_path = 'CarData/TrainImages/pos-%d.pgm' % (i+1)\n",
        "    neg_path = 'CarData/TrainImages/neg-%d.pgm' % (i+1)\n",
        "    return pos_path, neg_path\n",
        "\n",
        "def add_sample(path):\n",
        "    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
        "    keypoints, descriptors = sift.detectAndCompute(img, None)\n",
        "    if descriptors is not None:\n",
        "        bow_kmeans_trainer.add(descriptors)\n",
        "\n",
        "for i in range(BOW_NUM_TRAINING_SAMPLES_PER_CLASS):\n",
        "    pos_path, neg_path = get_pos_and_neg_paths(i)\n",
        "    add_sample(pos_path)\n",
        "    add_sample(neg_path)\n",
        "\n",
        "voc = bow_kmeans_trainer.cluster()\n",
        "bow_extractor.setVocabulary(voc)\n",
        "\n",
        "def extract_bow_descriptors(img):\n",
        "    features = sift.detect(img)\n",
        "    return bow_extractor.compute(img, features)\n",
        "\n",
        "training_data = []\n",
        "training_labels = []\n",
        "for i in range(SVM_NUM_TRAINING_SAMPLES_PER_CLASS):\n",
        "    pos_path, neg_path = get_pos_and_neg_paths(i)\n",
        "    pos_img = cv2.imread(pos_path, cv2.IMREAD_GRAYSCALE)\n",
        "    pos_descriptors = extract_bow_descriptors(pos_img)\n",
        "    if pos_descriptors is not None:\n",
        "        training_data.extend(pos_descriptors)\n",
        "        training_labels.append(1)\n",
        "    neg_img = cv2.imread(neg_path, cv2.IMREAD_GRAYSCALE)\n",
        "    neg_descriptors = extract_bow_descriptors(neg_img)\n",
        "    if neg_descriptors is not None:\n",
        "        training_data.extend(neg_descriptors)\n",
        "        training_labels.append(-1)\n",
        "\n",
        "svm = cv2.ml.SVM_create()\n",
        "svm.setType(cv2.ml.SVM_C_SVC)\n",
        "svm.setC(50)\n",
        "svm.train(np.array(training_data), cv2.ml.ROW_SAMPLE,\n",
        "          np.array(training_labels))\n",
        "\n",
        "def pyramid(img, scale_factor=1.25, min_size=(200, 80),\n",
        "            max_size=(600, 600)):\n",
        "    h, w = img.shape\n",
        "    min_w, min_h = min_size\n",
        "    max_w, max_h = max_size\n",
        "    while w >= min_w and h >= min_h:\n",
        "        if w <= max_w and h <= max_h:\n",
        "            yield img\n",
        "        w /= scale_factor\n",
        "        h /= scale_factor\n",
        "        img = cv2.resize(img, (int(w), int(h)),\n",
        "                         interpolation=cv2.INTER_AREA)\n",
        "\n",
        "def sliding_window(img, step=20, window_size=(100, 40)):\n",
        "    img_h, img_w = img.shape\n",
        "    window_w, window_h = window_size\n",
        "    for y in range(0, img_w, step):\n",
        "        for x in range(0, img_h, step):\n",
        "            roi = img[y:y+window_h, x:x+window_w]\n",
        "            roi_h, roi_w = roi.shape\n",
        "            if roi_w == window_w and roi_h == window_h:\n",
        "                yield (x, y, roi)\n",
        "\n",
        "for test_img_path in ['CarData/TestImages/test-0.pgm',\n",
        "                      'CarData/TestImages/test-1.pgm',\n",
        "                      '../images/car.jpg',\n",
        "                      '../images/haying.jpg',\n",
        "                      '../images/statue.jpg',\n",
        "                      '../images/woodcutters.jpg']:\n",
        "    img = cv2.imread(test_img_path)\n",
        "    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    pos_rects = []\n",
        "    for resized in pyramid(gray_img):\n",
        "        for x, y, roi in sliding_window(resized):\n",
        "            descriptors = extract_bow_descriptors(roi)\n",
        "            if descriptors is None:\n",
        "                continue\n",
        "            prediction = svm.predict(descriptors)\n",
        "            if prediction[1][0][0] == 1.0:\n",
        "                raw_prediction = svm.predict(\n",
        "                    descriptors, flags=cv2.ml.STAT_MODEL_RAW_OUTPUT)\n",
        "                score = -raw_prediction[1][0][0]\n",
        "                if score > SVM_SCORE_THRESHOLD:\n",
        "                    h, w = roi.shape\n",
        "                    scale = gray_img.shape[0] / float(resized.shape[0])\n",
        "                    pos_rects.append([int(x * scale),\n",
        "                                      int(y * scale),\n",
        "                                      int((x+w) * scale),\n",
        "                                      int((y+h) * scale),\n",
        "                                      score])\n",
        "    pos_rects = nms(np.array(pos_rects), NMS_OVERLAP_THRESHOLD)\n",
        "    for x0, y0, x1, y1, score in pos_rects:\n",
        "        cv2.rectangle(img, (int(x0), int(y0)), (int(x1), int(y1)),\n",
        "                      (0, 255, 255), 2)\n",
        "        text = '%.2f' % score\n",
        "        cv2.putText(img, text, (int(x0), int(y0) - 20),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
        "    cv2_imshow(img)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hj6yBNMy4l_M",
        "colab_type": "text"
      },
      "source": [
        "### Detect People with HOG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uBzKNyF4GPk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "\n",
        "def is_inside(i, o):\n",
        "    ix, iy, iw, ih = i\n",
        "    ox, oy, ow, oh = o\n",
        "    return ix > ox and ix + iw < ox + ow and \\\n",
        "        iy > oy and iy + ih < oy + oh\n",
        "\n",
        "hog = cv2.HOGDescriptor()\n",
        "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
        "\n",
        "img = cv2.imread('../images/haying.jpg')\n",
        "\n",
        "found_rects, found_weights = hog.detectMultiScale(\n",
        "    img, winStride=(4, 4), scale=1.02, finalThreshold=1.9)\n",
        "\n",
        "found_rects_filtered = []\n",
        "found_weights_filtered = []\n",
        "for ri, r in enumerate(found_rects):\n",
        "    for qi, q in enumerate(found_rects):\n",
        "        if ri != qi and is_inside(r, q):\n",
        "            break\n",
        "    else:\n",
        "        found_rects_filtered.append(r)\n",
        "        found_weights_filtered.append(found_weights[ri])\n",
        "\n",
        "for ri, r in enumerate(found_rects_filtered):\n",
        "    x, y, w, h = r\n",
        "    cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 255), 2)\n",
        "    text = '%.2f' % found_weights_filtered[ri]\n",
        "    cv2.putText(img, text, (x, y - 20),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
        "\n",
        "cv2_imshow(img)\n",
        "cv2.imwrite('./women_in_hayfield_detected.jpg', img)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-nmPuyI6eQs",
        "colab_type": "text"
      },
      "source": [
        "### GMG (does not work)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ge4xWHHk5nxc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cp -av \"/content/Learning-OpenCV-4-Computer-Vision-with-Python-Third-Edition/chapter08/traffic.flv\" \"/content/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ktsah8LR4s1x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "\n",
        "OPENCV_MAJOR_VERSION = int(cv2.__version__.split('.')[0])\n",
        "\n",
        "bg_subtractor = cv2.bgsegm.createBackgroundSubtractorGMG()\n",
        "\n",
        "erode_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (13, 9))\n",
        "dilate_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (17, 11))\n",
        "\n",
        "cap = cv2.VideoCapture('traffic.flv')\n",
        "success, frame = cap.read()\n",
        "while success:\n",
        "\n",
        "    fg_mask = bg_subtractor.apply(frame)\n",
        "\n",
        "    _, thresh = cv2.threshold(fg_mask, 244, 255, cv2.THRESH_BINARY)\n",
        "    cv2.erode(thresh, erode_kernel, thresh, iterations=2)\n",
        "    cv2.dilate(thresh, dilate_kernel, thresh, iterations=2)\n",
        "\n",
        "    if OPENCV_MAJOR_VERSION >= 4:\n",
        "        # OpenCV 4 or a later version is being used.\n",
        "        contours, hier = cv2.findContours(thresh, cv2.RETR_EXTERNAL,\n",
        "                                          cv2.CHAIN_APPROX_SIMPLE)\n",
        "    else:\n",
        "        # OpenCV 3 or an earlier version is being used.\n",
        "        # cv2.findContours has an extra return value.\n",
        "        # The extra return value is the thresholded image, which is\n",
        "        # unchanged, so we can ignore it.\n",
        "        _, contours, hier = cv2.findContours(thresh, cv2.RETR_EXTERNAL,\n",
        "                                             cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    for c in contours:\n",
        "        if cv2.contourArea(c) > 1000:\n",
        "            x, y, w, h = cv2.boundingRect(c)\n",
        "            cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 255, 0), 2)\n",
        "\n",
        "    cv2_imshow(fg_mask)\n",
        "    cv2_imshow(thresh)\n",
        "    cv2_imshow(frame)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEpxQ2Yb7XsS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTRraqmE7Y5n",
        "colab_type": "text"
      },
      "source": [
        "### Kallman filters (does not work)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nV6IXkQq5xlo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Create a black image.\n",
        "img = np.zeros((800, 800, 3), np.uint8)\n",
        "\n",
        "# Initialize the Kalman filter.\n",
        "kalman = cv2.KalmanFilter(4, 2)\n",
        "kalman.measurementMatrix = np.array(\n",
        "    [[1, 0, 0, 0],\n",
        "     [0, 1, 0, 0]], np.float32)\n",
        "kalman.transitionMatrix = np.array(\n",
        "    [[1, 0, 1, 0],\n",
        "     [0, 1, 0, 1],\n",
        "     [0, 0, 1, 0],\n",
        "     [0, 0, 0, 1]], np.float32)\n",
        "kalman.processNoiseCov = np.array(\n",
        "    [[1, 0, 0, 0],\n",
        "     [0, 1, 0, 0],\n",
        "     [0, 0, 1, 0],\n",
        "     [0, 0, 0, 1]], np.float32) * 0.03\n",
        "\n",
        "last_measurement = None\n",
        "last_prediction = None\n",
        "\n",
        "def on_mouse_moved(event, x, y, flags, param):\n",
        "    global img, kalman, last_measurement, last_prediction\n",
        "\n",
        "    measurement = np.array([[x], [y]], np.float32)\n",
        "    if last_measurement is None:\n",
        "        # This is the first measurement.\n",
        "        # Update the Kalman filter's state to match the measurement.\n",
        "        kalman.statePre = np.array(\n",
        "            [[x], [y], [0], [0]], np.float32)\n",
        "        kalman.statePost = np.array(\n",
        "            [[x], [y], [0], [0]], np.float32)\n",
        "        prediction = measurement\n",
        "    else:\n",
        "        kalman.correct(measurement)\n",
        "        prediction = kalman.predict()  # Gets a reference, not a copy\n",
        "\n",
        "        # Trace the path of the measurement in green.\n",
        "        cv2.line(img, (last_measurement[0], last_measurement[1]),\n",
        "                 (measurement[0], measurement[1]), (0, 255, 0))\n",
        "\n",
        "        # Trace the path of the prediction in red.\n",
        "        cv2.line(img, (last_prediction[0], last_prediction[1]),\n",
        "                 (prediction[0], prediction[1]), (0, 0, 255))\n",
        "\n",
        "    last_prediction = prediction.copy()\n",
        "    last_measurement = measurement\n",
        "\n",
        "cv2.namedWindow('kalman_tracker')\n",
        "cv2.setMouseCallback('kalman_tracker', on_mouse_moved)\n",
        "\n",
        "while True:\n",
        "    cv2_imshow(img)\n",
        "    cv2.imwrite('kalman.png', img)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNfyT2Wc8UiX",
        "colab_type": "text"
      },
      "source": [
        "### BackgroundSubtractorKNN does not work"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfOKhrQX7_-a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "\n",
        "OPENCV_MAJOR_VERSION = int(cv2.__version__.split('.')[0])\n",
        "\n",
        "bg_subtractor = cv2.createBackgroundSubtractorKNN(detectShadows=True)\n",
        "\n",
        "erode_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (7, 5))\n",
        "dilate_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (17, 11))\n",
        "\n",
        "cap = cv2.VideoCapture('traffic.flv')\n",
        "success, frame = cap.read()\n",
        "while success:\n",
        "\n",
        "    fg_mask = bg_subtractor.apply(frame)\n",
        "\n",
        "    _, thresh = cv2.threshold(fg_mask, 244, 255, cv2.THRESH_BINARY)\n",
        "    cv2.erode(thresh, erode_kernel, thresh, iterations=2)\n",
        "    cv2.dilate(thresh, dilate_kernel, thresh, iterations=2)\n",
        "\n",
        "    if OPENCV_MAJOR_VERSION >= 4:\n",
        "        # OpenCV 4 or a later version is being used.\n",
        "        contours, hier = cv2.findContours(thresh, cv2.RETR_EXTERNAL,\n",
        "                                          cv2.CHAIN_APPROX_SIMPLE)\n",
        "    else:\n",
        "        # OpenCV 3 or an earlier version is being used.\n",
        "        # cv2.findContours has an extra return value.\n",
        "        # The extra return value is the thresholded image, which is\n",
        "        # unchanged, so we can ignore it.\n",
        "        _, contours, hier = cv2.findContours(thresh, cv2.RETR_EXTERNAL,\n",
        "                                             cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    for c in contours:\n",
        "        if cv2.contourArea(c) > 1000:\n",
        "            x, y, w, h = cv2.boundingRect(c)\n",
        "            cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 255, 0), 2)\n",
        "\n",
        "    cv2.imshow('knn', fg_mask)\n",
        "    cv2.imshow('thresh', thresh)\n",
        "    cv2.imshow('detection', frame)\n",
        "\n",
        "    k = cv2.waitKey(30)\n",
        "    if k == 27:  # Escape\n",
        "        break\n",
        "\n",
        "    success, frame = cap.read()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMO4JCPs898n",
        "colab_type": "text"
      },
      "source": [
        "### the Lucas-Kanade optical flow (does not work)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoB7dGCf8rFS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "# Capture several frames to allow the camera's autoexposure to adjust.\n",
        "for i in range(10):\n",
        "    success, old_frame = cap.read()\n",
        "if not success:\n",
        "    exit(1)\n",
        "\n",
        "# Find the initial features using Shi-Tomasi corner detection.\n",
        "old_gray_frame = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)\n",
        "old_points = cv2.goodFeaturesToTrack(\n",
        "    old_gray_frame, maxCorners=100, qualityLevel=0.3, minDistance=7,\n",
        "    blockSize=7)\n",
        "\n",
        "# Define the Lukas-Kanade optical flow's termination criteria:\n",
        "# 10 iterations or convergence within 0.03-pixel radius.\n",
        "term_crit = (cv2.TERM_CRITERIA_COUNT | cv2.TERM_CRITERIA_EPS,\n",
        "             10, 0.03)\n",
        "\n",
        "# Create an overlay to use in drawing motion trails.\n",
        "overlay = np.zeros_like(old_frame)\n",
        "\n",
        "# Create random colors to use in drawing motion trails.\n",
        "colors = np.random.randint(0, 255, (100, 3))\n",
        "\n",
        "success, frame = cap.read()\n",
        "while(success):\n",
        "\n",
        "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Calculate the Lucas-Kanade optical flow.\n",
        "    points, statuses, distances = cv2.calcOpticalFlowPyrLK(\n",
        "         old_gray_frame, gray_frame, old_points, None,\n",
        "         winSize=(15, 15), maxLevel=2, criteria=term_crit)\n",
        "\n",
        "    # Select the points that were successfully tracked.\n",
        "    good_points = points[statuses==1]\n",
        "    good_old_points = old_points[statuses==1]\n",
        "\n",
        "    # Draw and show the motion trails.\n",
        "    for i, (point, old_point) in enumerate(\n",
        "            zip(good_points, good_old_points)):\n",
        "        a, b = point.ravel()\n",
        "        c, d = old_point.ravel()\n",
        "        color = colors[i].tolist()\n",
        "        cv2.line(overlay, (a, b), (c, d), color, 2)\n",
        "        cv2.circle(frame, (a, b), 5, color, cv2.FILLED)\n",
        "    cv2.add(frame, overlay, frame)\n",
        "    cv2.imshow('lk', frame)\n",
        "\n",
        "    k = cv2.waitKey(1)\n",
        "    if k == 27:  # Escape\n",
        "        break\n",
        "\n",
        "    # Update the previous frame and points.\n",
        "    old_gray_frame = gray_frame\n",
        "    old_points = good_points.reshape(-1, 1, 2)\n",
        "\n",
        "    success, frame = cap.read()\n",
        "\n",
        "cv2.destroyAllWindows()\n",
        "cap.release()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7B8-vCxj9i9-",
        "colab_type": "text"
      },
      "source": [
        "### Perform tracking with MeanShift (does not work)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAcYGw_x9R2i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "# Capture several frames to allow the camera's autoexposure to adjust.\n",
        "for i in range(10):\n",
        "    success, frame = cap.read()\n",
        "if not success:\n",
        "    exit(1)\n",
        "\n",
        "# Define an initial tracking window in the center of the frame.\n",
        "frame_h, frame_w = frame.shape[:2]\n",
        "w = frame_w//8\n",
        "h = frame_h//8\n",
        "x = frame_w//2 - w//2\n",
        "y = frame_h//2 - h//2\n",
        "track_window = (x, y, w, h)\n",
        "\n",
        "# Calculate the normalized HSV histogram of the initial window.\n",
        "roi = frame[y:y+h, x:x+w]\n",
        "hsv_roi =  cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n",
        "mask = None\n",
        "roi_hist = cv2.calcHist([hsv_roi], [0], mask, [180], [0, 180])\n",
        "cv2.normalize(roi_hist, roi_hist, 0, 255, cv2.NORM_MINMAX)\n",
        "\n",
        "# Define the termination criteria:\n",
        "# 10 iterations or convergence within 1-pixel radius.\n",
        "term_crit = (cv2.TERM_CRITERIA_COUNT | cv2.TERM_CRITERIA_EPS, 10, 1)\n",
        "\n",
        "success, frame = cap.read()\n",
        "while success:\n",
        "\n",
        "    # Perform back-projection of the HSV histogram onto the frame.\n",
        "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
        "    back_proj = cv2.calcBackProject([hsv], [0], roi_hist, [0, 180], 1)\n",
        "\n",
        "    # Perform tracking with MeanShift.\n",
        "    num_iters, track_window = cv2.meanShift(\n",
        "        back_proj, track_window, term_crit)\n",
        "\n",
        "    # Draw the tracking window.\n",
        "    x, y, w, h = track_window\n",
        "    cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
        "\n",
        "    cv2.imshow('back-projection', back_proj)\n",
        "    cv2.imshow('meanshift', frame)\n",
        "\n",
        "    k = cv2.waitKey(1)\n",
        "    if k == 27:  # Escape\n",
        "        break\n",
        "\n",
        "    success, frame = cap.read()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Hy62KyJ-BWm",
        "colab_type": "text"
      },
      "source": [
        "### MOG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXRUwZ-L96l5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "\n",
        "OPENCV_MAJOR_VERSION = int(cv2.__version__.split('.')[0])\n",
        "\n",
        "bg_subtractor = cv2.createBackgroundSubtractorMOG2(detectShadows=True)\n",
        "\n",
        "erode_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
        "dilate_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (7, 7))\n",
        "\n",
        "cap = cv2.VideoCapture('hallway.mpg')\n",
        "success, frame = cap.read()\n",
        "while success:\n",
        "\n",
        "    fg_mask = bg_subtractor.apply(frame)\n",
        "\n",
        "    _, thresh = cv2.threshold(fg_mask, 244, 255, cv2.THRESH_BINARY)\n",
        "    cv2.erode(thresh, erode_kernel, thresh, iterations=2)\n",
        "    cv2.dilate(thresh, dilate_kernel, thresh, iterations=2)\n",
        "\n",
        "    if OPENCV_MAJOR_VERSION >= 4:\n",
        "        # OpenCV 4 or a later version is being used.\n",
        "        contours, hier = cv2.findContours(thresh, cv2.RETR_EXTERNAL,\n",
        "                                          cv2.CHAIN_APPROX_SIMPLE)\n",
        "    else:\n",
        "        # OpenCV 3 or an earlier version is being used.\n",
        "        # cv2.findContours has an extra return value.\n",
        "        # The extra return value is the thresholded image, which is\n",
        "        # unchanged, so we can ignore it.\n",
        "        _, contours, hier = cv2.findContours(thresh, cv2.RETR_EXTERNAL,\n",
        "                                             cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    for c in contours:\n",
        "        if cv2.contourArea(c) > 1000:\n",
        "            x, y, w, h = cv2.boundingRect(c)\n",
        "            cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 255, 0), 2)\n",
        "\n",
        "    cv2.imshow('mog', fg_mask)\n",
        "    cv2.imshow('thresh', thresh)\n",
        "    cv2.imshow('detection', frame)\n",
        "\n",
        "    k = cv2.waitKey(30)\n",
        "    if k == 27:  # Escape\n",
        "        break\n",
        "\n",
        "    success, frame = cap.read()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chA__S20-Wj-",
        "colab_type": "text"
      },
      "source": [
        "### Tracked pedestrian with a state"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xlq9kM7-Q0-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "OPENCV_MAJOR_VERSION = int(cv2.__version__.split('.')[0])\n",
        "\n",
        "class Pedestrian():\n",
        "    \"\"\"A tracked pedestrian with a state including an ID, tracking\n",
        "    window, histogram, and Kalman filter.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, id, hsv_frame, track_window):\n",
        "\n",
        "        self.id = id\n",
        "\n",
        "        self.track_window = track_window\n",
        "        self.term_crit = \\\n",
        "            (cv2.TERM_CRITERIA_COUNT | cv2.TERM_CRITERIA_EPS, 10, 1)\n",
        "\n",
        "        # Initialize the histogram.\n",
        "        x, y, w, h = track_window\n",
        "        roi = hsv_frame[y:y+h, x:x+w]\n",
        "        roi_hist = cv2.calcHist([roi], [0], None, [16], [0, 180])\n",
        "        self.roi_hist = cv2.normalize(roi_hist, roi_hist, 0, 255,\n",
        "                                      cv2.NORM_MINMAX)\n",
        "\n",
        "        # Initialize the Kalman filter.\n",
        "        self.kalman = cv2.KalmanFilter(4, 2)\n",
        "        self.kalman.measurementMatrix = np.array(\n",
        "            [[1, 0, 0, 0],\n",
        "             [0, 1, 0, 0]], np.float32)\n",
        "        self.kalman.transitionMatrix = np.array(\n",
        "            [[1, 0, 1, 0],\n",
        "             [0, 1, 0, 1],\n",
        "             [0, 0, 1, 0],\n",
        "             [0, 0, 0, 1]], np.float32)\n",
        "        self.kalman.processNoiseCov = np.array(\n",
        "            [[1, 0, 0, 0],\n",
        "             [0, 1, 0, 0],\n",
        "             [0, 0, 1, 0],\n",
        "             [0, 0, 0, 1]], np.float32) * 0.03\n",
        "        cx = x+w/2\n",
        "        cy = y+h/2\n",
        "        self.kalman.statePre = np.array(\n",
        "            [[cx], [cy], [0], [0]], np.float32)\n",
        "        self.kalman.statePost = np.array(\n",
        "            [[cx], [cy], [0], [0]], np.float32)\n",
        "\n",
        "    def update(self, frame, hsv_frame):\n",
        "\n",
        "        back_proj = cv2.calcBackProject(\n",
        "            [hsv_frame], [0], self.roi_hist, [0, 180], 1)\n",
        "\n",
        "        ret, self.track_window = cv2.meanShift(\n",
        "            back_proj, self.track_window, self.term_crit)\n",
        "        x, y, w, h = self.track_window\n",
        "        center = np.array([x+w/2, y+h/2], np.float32)\n",
        "\n",
        "        prediction = self.kalman.predict()\n",
        "        estimate = self.kalman.correct(center)\n",
        "        center_offset = estimate[:,0][:2] - center\n",
        "        self.track_window = (x + int(center_offset[0]),\n",
        "                             y + int(center_offset[1]), w, h)\n",
        "        x, y, w, h = self.track_window\n",
        "\n",
        "        # Draw the predicted center position as a blue circle.\n",
        "        cv2.circle(frame, (int(prediction[0]), int(prediction[1])),\n",
        "                   4, (255, 0, 0), -1)\n",
        "\n",
        "        # Draw the corrected tracking window as a cyan rectangle.\n",
        "        cv2.rectangle(frame, (x,y), (x+w, y+h), (255, 255, 0), 2)\n",
        "\n",
        "        # Draw the ID above the rectangle in blue text.\n",
        "        cv2.putText(frame, 'ID: %d' % self.id, (x, y-5),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0),\n",
        "                    1, cv2.LINE_AA)\n",
        "\n",
        "def main():\n",
        "\n",
        "    cap = cv2.VideoCapture('pedestrians.avi')\n",
        "\n",
        "    # Create the KNN background subtractor.\n",
        "    bg_subtractor = cv2.createBackgroundSubtractorKNN()\n",
        "    history_length = 20\n",
        "    bg_subtractor.setHistory(history_length)\n",
        "\n",
        "    erode_kernel = cv2.getStructuringElement(\n",
        "        cv2.MORPH_ELLIPSE, (3, 3))\n",
        "    dilate_kernel = cv2.getStructuringElement(\n",
        "        cv2.MORPH_ELLIPSE, (8, 3))\n",
        "\n",
        "    pedestrians = []\n",
        "    num_history_frames_populated = 0\n",
        "    while True:\n",
        "        grabbed, frame = cap.read()\n",
        "        if (grabbed is False):\n",
        "            break\n",
        "\n",
        "        # Apply the KNN background subtractor.\n",
        "        fg_mask = bg_subtractor.apply(frame)\n",
        "\n",
        "        # Let the background subtractor build up a history.\n",
        "        if num_history_frames_populated < history_length:\n",
        "            num_history_frames_populated += 1\n",
        "            continue\n",
        "\n",
        "        # Create the thresholded image.\n",
        "        _, thresh = cv2.threshold(fg_mask, 127, 255,\n",
        "                                  cv2.THRESH_BINARY)\n",
        "        cv2.erode(thresh, erode_kernel, thresh, iterations=2)\n",
        "        cv2.dilate(thresh, dilate_kernel, thresh, iterations=2)\n",
        "\n",
        "        # Detect contours in the thresholded image.\n",
        "        if OPENCV_MAJOR_VERSION >= 4:\n",
        "            # OpenCV 4 or a later version is being used.\n",
        "            contours, hier = cv2.findContours(\n",
        "                thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        else:\n",
        "            # OpenCV 3 or an earlier version is being used.\n",
        "            # cv2.findContours has an extra return value.\n",
        "            # The extra return value is the thresholded image, which\n",
        "            # is unchanged, so we can ignore it.\n",
        "            _, contours, hier = cv2.findContours(\n",
        "                thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "        hsv_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "        # Draw green rectangles around large contours.\n",
        "        # Also, if no pedestrians are being tracked yet, create some.\n",
        "        should_initialize_pedestrians = len(pedestrians) == 0\n",
        "        id = 0\n",
        "        for c in contours:\n",
        "            if cv2.contourArea(c) > 500:\n",
        "                (x, y, w, h) = cv2.boundingRect(c)\n",
        "                cv2.rectangle(frame, (x, y), (x+w, y+h),\n",
        "                              (0, 255, 0), 1)\n",
        "                if should_initialize_pedestrians:\n",
        "                    pedestrians.append(\n",
        "                        Pedestrian(id, hsv_frame,\n",
        "                                   (x, y, w, h)))\n",
        "            id += 1\n",
        "\n",
        "        # Update the tracking of each pedestrian.\n",
        "        for pedestrian in pedestrians:\n",
        "            pedestrian.update(frame, hsv_frame)\n",
        "\n",
        "        cv2.imshow('Pedestrians Tracked', frame)\n",
        "\n",
        "        k = cv2.waitKey(110)\n",
        "        if k == 27:  # Escape\n",
        "            break\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBH6HFEh_KNl",
        "colab_type": "text"
      },
      "source": [
        "### Simple ANN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbSSX1D8_CuA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "ann = cv2.ml.ANN_MLP_create()\n",
        "ann.setLayerSizes(np.array([9, 15, 9], np.uint8))\n",
        "ann.setActivationFunction(cv2.ml.ANN_MLP_SIGMOID_SYM, 0.6, 1.0)\n",
        "ann.setTrainMethod(cv2.ml.ANN_MLP_BACKPROP, 0.1, 0.1)\n",
        "ann.setTermCriteria(\n",
        "    (cv2.TERM_CRITERIA_MAX_ITER | cv2.TERM_CRITERIA_EPS, 100, 1.0))\n",
        "\n",
        "training_samples = np.array(\n",
        "    [[1.2, 1.3, 1.9, 2.2, 2.3, 2.9, 3.0, 3.2, 3.3]], np.float32)\n",
        "layout = cv2.ml.ROW_SAMPLE\n",
        "training_responses = np.array(\n",
        "    [[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]], np.float32)\n",
        "data = cv2.ml.TrainData_create(\n",
        "    training_samples, layout, training_responses)\n",
        "ann.train(data)\n",
        "\n",
        "test_samples = np.array(\n",
        "    [[1.4, 1.5, 1.2, 2.0, 2.5, 2.8, 3.0, 3.1, 3.8]], np.float32)\n",
        "prediction = ann.predict(test_samples)\n",
        "print(prediction)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytxHL2RG_2tF",
        "colab_type": "text"
      },
      "source": [
        "### Animals ANN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYRW9Y6l_zb5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from random import randint, uniform\n",
        "\n",
        "animals_net = cv2.ml.ANN_MLP_create()\n",
        "animals_net.setLayerSizes(np.array([3, 50, 4]))\n",
        "animals_net.setActivationFunction(cv2.ml.ANN_MLP_SIGMOID_SYM, 0.6, 1.0)\n",
        "animals_net.setTrainMethod(cv2.ml.ANN_MLP_BACKPROP, 0.1, 0.1)\n",
        "animals_net.setTermCriteria(\n",
        "    (cv2.TERM_CRITERIA_MAX_ITER | cv2.TERM_CRITERIA_EPS, 100, 1.0))\n",
        "\n",
        "\"\"\"Input arrays\n",
        "weight, length, teeth\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"Output arrays\n",
        "dog, condor, dolphin, dragon\n",
        "\"\"\"\n",
        "\n",
        "def dog_sample():\n",
        "    return [uniform(10.0, 20.0), uniform(1.0, 1.5), randint(38, 42)]\n",
        "\n",
        "def dog_class():\n",
        "    return [1, 0, 0, 0]\n",
        "\n",
        "def condor_sample():\n",
        "    return [uniform(3.0, 10.0), randint(3.0, 5.0), 0]\n",
        "\n",
        "def condor_class():\n",
        "    return [0, 1, 0, 0]\n",
        "\n",
        "def dolphin_sample():\n",
        "    return [uniform(30.0, 190.0), uniform(5.0, 15.0), randint(80, 100)]\n",
        "\n",
        "def dolphin_class():\n",
        "    return [0, 0, 1, 0]\n",
        "\n",
        "def dragon_sample():\n",
        "    return [uniform(1200.0, 1800.0), uniform(30.0, 40.0), randint(160, 180)]\n",
        "\n",
        "def dragon_class():\n",
        "    return [0, 0, 0, 1]\n",
        "\n",
        "def record(sample, classification):\n",
        "    return (np.array([sample], np.float32),\n",
        "            np.array([classification], np.float32))\n",
        "\n",
        "\n",
        "RECORDS = 20000\n",
        "records = []\n",
        "for x in range(0, RECORDS):\n",
        "    records.append(record(dog_sample(), dog_class()))\n",
        "    records.append(record(condor_sample(), condor_class()))\n",
        "    records.append(record(dolphin_sample(), dolphin_class()))\n",
        "    records.append(record(dragon_sample(), dragon_class()))\n",
        "\n",
        "EPOCHS = 10\n",
        "for e in range(0, EPOCHS):\n",
        "    print(\"epoch: %d\" % e)\n",
        "    for t, c in records:\n",
        "        data = cv2.ml.TrainData_create(t, cv2.ml.ROW_SAMPLE, c)\n",
        "        if animals_net.isTrained():\n",
        "            animals_net.train(data, cv2.ml.ANN_MLP_UPDATE_WEIGHTS | cv2.ml.ANN_MLP_NO_INPUT_SCALE | cv2.ml.ANN_MLP_NO_OUTPUT_SCALE)\n",
        "        else:\n",
        "            animals_net.train(data, cv2.ml.ANN_MLP_NO_INPUT_SCALE | cv2.ml.ANN_MLP_NO_OUTPUT_SCALE)\n",
        "\n",
        "\n",
        "TESTS = 100\n",
        "\n",
        "dog_results = 0\n",
        "for x in range(0, TESTS):\n",
        "    clas = int(animals_net.predict(\n",
        "        np.array([dog_sample()], np.float32))[0])\n",
        "    print(\"class: %d\" % clas)\n",
        "    if clas == 0:\n",
        "        dog_results += 1\n",
        "\n",
        "condor_results = 0\n",
        "for x in range(0, TESTS):\n",
        "    clas = int(animals_net.predict(\n",
        "        np.array([condor_sample()], np.float32))[0])\n",
        "    print(\"class: %d\" % clas)\n",
        "    if clas == 1:\n",
        "        condor_results += 1\n",
        "\n",
        "dolphin_results = 0\n",
        "for x in range(0, TESTS):\n",
        "    clas = int(animals_net.predict(\n",
        "        np.array([dolphin_sample()], np.float32))[0])\n",
        "    print(\"class: %d\" % clas)\n",
        "    if clas == 2:\n",
        "        dolphin_results += 1\n",
        "\n",
        "dragon_results = 0\n",
        "for x in range(0, TESTS):\n",
        "    clas = int(animals_net.predict(\n",
        "        np.array([dragon_sample()], np.float32))[0])\n",
        "    print(\"class: %d\" % clas)\n",
        "    if clas == 3:\n",
        "        dragon_results += 1\n",
        "\n",
        "print(\"dog accuracy: %.2f%%\" % (100.0 * dog_results / TESTS))\n",
        "print(\"condor accuracy: %.2f%%\" % (100.0 * condor_results / TESTS))\n",
        "print(\"dolphin accuracy: %.2f%%\" % (100.0 * dolphin_results / TESTS))\n",
        "print(\"dragon accuracy: %.2f%%\" % (100.0 * dragon_results / TESTS))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOUShWXNAgEi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cp -av \"/content/Learning-OpenCV-4-Computer-Vision-with-Python-Third-Edition/chapter10/digits_data\" \"/content/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AB8ASsL1ASO_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gzip\n",
        "import pickle\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\"\"\"OpenCV ANN Handwritten digit recognition example\n",
        "\n",
        "Wraps OpenCV's own ANN by automating the loading of data and supplying default paramters,\n",
        "such as 20 hidden layers, 10000 samples and 1 training epoch.\n",
        "\n",
        "The load data code is adapted from http://neuralnetworksanddeeplearning.com/chap1.html\n",
        "by Michael Nielsen\n",
        "\"\"\"\n",
        "\n",
        "def load_data():\n",
        "    mnist = gzip.open('/content/digits_data/mnist.pkl.gz', 'rb')\n",
        "    training_data, test_data = pickle.load(mnist)\n",
        "    mnist.close()\n",
        "    return (training_data, test_data)\n",
        "\n",
        "def wrap_data():\n",
        "    tr_d, te_d = load_data()\n",
        "    training_inputs = tr_d[0]\n",
        "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
        "    training_data = zip(training_inputs, training_results)\n",
        "    test_data = zip(te_d[0], te_d[1])\n",
        "    return (training_data, test_data)\n",
        "\n",
        "def vectorized_result(j):\n",
        "    e = np.zeros((10,), np.float32)\n",
        "    e[j] = 1.0\n",
        "    return e\n",
        "\n",
        "def create_ann(hidden_nodes=60):\n",
        "    ann = cv2.ml.ANN_MLP_create()\n",
        "    ann.setLayerSizes(np.array([784, hidden_nodes, 10]))\n",
        "    ann.setActivationFunction(cv2.ml.ANN_MLP_SIGMOID_SYM, 0.6, 1.0)\n",
        "    ann.setTrainMethod(cv2.ml.ANN_MLP_BACKPROP, 0.1, 0.1)\n",
        "    ann.setTermCriteria(\n",
        "        (cv2.TERM_CRITERIA_MAX_ITER | cv2.TERM_CRITERIA_EPS, 100, 1.0))\n",
        "    return ann\n",
        "\n",
        "def train(ann, samples=50000, epochs=10):\n",
        "\n",
        "    tr, test = wrap_data()\n",
        "\n",
        "    # Convert iterator to list so that we can iterate multiple times\n",
        "    # in multiple epochs.\n",
        "    tr = list(tr)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(\"Completed %d/%d epochs\" % (epoch, epochs))\n",
        "        counter = 0\n",
        "        for img in tr:\n",
        "            if (counter > samples):\n",
        "                break\n",
        "            if (counter % 1000 == 0):\n",
        "                print(\"Epoch %d: Trained on %d/%d samples\" % \\\n",
        "                      (epoch, counter, samples))\n",
        "            counter += 1\n",
        "            sample, response = img\n",
        "            data = cv2.ml.TrainData_create(\n",
        "                np.array([sample], dtype=np.float32),\n",
        "                cv2.ml.ROW_SAMPLE,\n",
        "                np.array([response], dtype=np.float32))\n",
        "            if ann.isTrained():\n",
        "                ann.train(data, cv2.ml.ANN_MLP_UPDATE_WEIGHTS | cv2.ml.ANN_MLP_NO_INPUT_SCALE | cv2.ml.ANN_MLP_NO_OUTPUT_SCALE)\n",
        "            else:\n",
        "                ann.train(data, cv2.ml.ANN_MLP_NO_INPUT_SCALE | cv2.ml.ANN_MLP_NO_OUTPUT_SCALE)\n",
        "    print(\"Completed all epochs!\")\n",
        "\n",
        "    return ann, test\n",
        "\n",
        "def test(ann, test_data):\n",
        "    num_tests = 0\n",
        "    num_correct = 0\n",
        "    for img in test_data:\n",
        "        num_tests += 1\n",
        "        sample, correct_digit_class = img\n",
        "        digit_class = predict(ann, sample)[0]\n",
        "        if digit_class == correct_digit_class:\n",
        "            num_correct += 1\n",
        "    print('Accuracy: %.2f%%' % (100.0 * num_correct / num_tests))\n",
        "\n",
        "def predict(ann, sample):\n",
        "    if sample.shape != (784,):\n",
        "        if sample.shape != (28, 28):\n",
        "            sample = cv2.resize(sample, (28, 28),\n",
        "                                interpolation=cv2.INTER_LINEAR)\n",
        "        sample = sample.reshape(784,)\n",
        "    return ann.predict(np.array([sample], dtype=np.float32))\n",
        "\n",
        "\n",
        "ann, test_data = train(create_ann())\n",
        "test(ann, test_data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdXiv632CHxE",
        "colab_type": "text"
      },
      "source": [
        "### Object DNN form Caffe model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZlMopbjBIiw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "model = cv2.dnn.readNetFromCaffe(\n",
        "    'objects_data/MobileNetSSD_deploy.prototxt',\n",
        "    'objects_data/MobileNetSSD_deploy.caffemodel')\n",
        "blob_height = 300\n",
        "color_scale = 1.0/127.5\n",
        "average_color = (127.5, 127.5, 127.5)\n",
        "confidence_threshold = 0.5\n",
        "labels = ['airplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n",
        "          'car', 'cat', 'chair', 'cow', 'dining table', 'dog',\n",
        "          'horse', 'motorbike', 'person', 'potted plant', 'sheep',\n",
        "          'sofa', 'train', 'TV or monitor']\n",
        "\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "success, frame = cap.read()\n",
        "while success:\n",
        "\n",
        "    h, w = frame.shape[:2]\n",
        "    aspect_ratio = w/h\n",
        "\n",
        "    # Detect objects in the frame.\n",
        "\n",
        "    blob_width = int(blob_height * aspect_ratio)\n",
        "    blob_size = (blob_width, blob_height)\n",
        "\n",
        "    blob = cv2.dnn.blobFromImage(\n",
        "        frame, scalefactor=color_scale, size=blob_size,\n",
        "        mean=average_color)\n",
        "\n",
        "    model.setInput(blob)\n",
        "    results = model.forward()\n",
        "\n",
        "    # Iterate over the detected objects.\n",
        "    for object in results[0, 0]:\n",
        "        confidence = object[2]\n",
        "        if confidence > confidence_threshold:\n",
        "\n",
        "            # Get the object's coordinates.\n",
        "            x0, y0, x1, y1 = (object[3:7] * [w, h, w, h]).astype(int)\n",
        "\n",
        "            # Get the classification result.\n",
        "            id = int(object[1])\n",
        "            label = labels[id - 1]\n",
        "\n",
        "            # Draw a blue rectangle around the object.\n",
        "            cv2.rectangle(frame, (x0, y0), (x1, y1),\n",
        "                          (255, 0, 0), 2)\n",
        "\n",
        "            # Draw the classification result and confidence.\n",
        "            text = '%s (%.1f%%)' % (label, confidence * 100.0)\n",
        "            cv2.putText(frame, text, (x0, y0 - 20),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
        "\n",
        "    cv2.imshow('Objects', frame)\n",
        "\n",
        "    k = cv2.waitKey(1)\n",
        "    if k == 27:  # Escape\n",
        "        break\n",
        "\n",
        "    success, frame = cap.read()\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}