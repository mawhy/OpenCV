{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Object Detection",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mawhy/OpenCV/blob/master/Object_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhIn1UVc1emx",
        "colab_type": "text"
      },
      "source": [
        "# Image Processing CookBook\n",
        "## Object Detection\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXavdjBft2fq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/PacktPublishing/Python-Image-Processing-Cookbook.git\n",
        "%cp -av \"/content/Python-Image-Processing-Cookbook/Chapter 08/images/\" \"/content/\"\n",
        "%cp -av \"/content/Python-Image-Processing-Cookbook/Chapter 08/models/\" \"/content/\"\n",
        "%rm -rf \"/content/Python-Image-Processing-Cookbook\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDj8RD9k1emz",
        "colab_type": "text"
      },
      "source": [
        "### People Detection with HOG/SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8YZyCM61emz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "img = cv2.imread(\"images/walk.png\")\n",
        "# create HOG descriptor using default people (pedestrian) detector\n",
        "hog = cv2.HOGDescriptor()\n",
        "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
        "# run detection, using a spatial stride of 4 pixels (horizontal and vertical), a scale stride of 1.02, and zero grouping of rectangles (to\n",
        "# demonstrate that HOG will detect at potentially multiple places in the scale pyramid)\n",
        "(found_bounding_boxes, weights) = hog.detectMultiScale(img, winStride=(4, 4), padding=(8, 8), scale=1.1, finalThreshold=0)\n",
        "print(len(found_bounding_boxes)) # number of boundingboxes\n",
        "# 314\n",
        "# copy the original image to draw bounding boxes on it for now, as we'll use it again later\n",
        "img_with_waw_bboxes = img.copy()\n",
        "for (hx, hy, hw, hh) in found_bounding_boxes:\n",
        "    cv2.rectangle(img_with_waw_bboxes, (hx, hy), (hx + hw, hy + hh), (0, 0, 255), 2)\n",
        "plt.figure(figsize=(20, 12))\n",
        "img_with_waw_bboxes = cv2.cvtColor(img_with_waw_bboxes, cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(img_with_waw_bboxes, aspect='auto'), plt.axis('off')\n",
        "plt.title('Boundingboxes found by HOG-SVM without grouping', size=20)\n",
        "plt.show()\n",
        "\n",
        "#https://gist.github.com/CMCDragonkai/1be3402e261d3c239a307a3346360506\n",
        "def non_max_suppression(boxes, scores, threshold):\t\n",
        "    assert boxes.shape[0] == scores.shape[0]\n",
        "    # bottom-left origin\n",
        "    ys1 = boxes[:, 0]\n",
        "    xs1 = boxes[:, 1]\n",
        "    # top-right target\n",
        "    ys2 = boxes[:, 2]\n",
        "    xs2 = boxes[:, 3]\n",
        "    # box coordinate ranges are inclusive-inclusive\n",
        "    areas = (ys2 - ys1) * (xs2 - xs1)\n",
        "    scores_indexes = scores.argsort().tolist()\n",
        "    boxes_keep_index = []\n",
        "    while len(scores_indexes):\n",
        "        index = scores_indexes.pop()\n",
        "        boxes_keep_index.append(index)\n",
        "        if not len(scores_indexes):\n",
        "            break\n",
        "        ious = compute_iou(boxes[index], boxes[scores_indexes], areas[index],\n",
        "                           areas[scores_indexes])\n",
        "        filtered_indexes = set((ious > threshold).nonzero()[0])\n",
        "        # if there are no more scores_index\n",
        "        # then we should pop it\n",
        "        scores_indexes = [\n",
        "            v for (i, v) in enumerate(scores_indexes)\n",
        "            if i not in filtered_indexes\n",
        "        ]\n",
        "    return np.array(boxes_keep_index)\n",
        "\n",
        "\n",
        "def compute_iou(box, boxes, box_area, boxes_area):\n",
        "    # this is the iou of the box against all other boxes\n",
        "    assert boxes.shape[0] == boxes_area.shape[0]\n",
        "    # get all the origin-ys\n",
        "    # push up all the lower origin-xs, while keeping the higher origin-xs\n",
        "    ys1 = np.maximum(box[0], boxes[:, 0])\n",
        "    # get all the origin-xs\n",
        "    # push right all the lower origin-xs, while keeping higher origin-xs\n",
        "    xs1 = np.maximum(box[1], boxes[:, 1])\n",
        "    # get all the target-ys\n",
        "    # pull down all the higher target-ys, while keeping lower origin-ys\n",
        "    ys2 = np.minimum(box[2], boxes[:, 2])\n",
        "    # get all the target-xs\n",
        "    # pull left all the higher target-xs, while keeping lower target-xs\n",
        "    xs2 = np.minimum(box[3], boxes[:, 3])\n",
        "    # each intersection area is calculated by the  pulled target-x minus the pushed origin-x\n",
        "    # multiplying pulled target-y minus the pushed origin-y \n",
        "    # we ignore areas where the intersection side would be negative\n",
        "    # this is done by using maxing the side length by 0\n",
        "    intersections = np.maximum(ys2 - ys1, 0) * np.maximum(xs2 - xs1, 0)\n",
        "    # each union is then the box area added to each other box area minusing their intersection calculated above\n",
        "    unions = box_area + boxes_area - intersections\n",
        "    # element wise division\n",
        "    # if the intersection is 0, then their ratio is 0\n",
        "    ious = intersections / unions\n",
        "    return ious\n",
        "\n",
        "(found_bounding_boxes, weights) = hog.detectMultiScale(img, winStride=(4, 4), padding=(8, 8), scale=1.1, finalThreshold=0)\n",
        "print(len(found_bounding_boxes)) # number of boundingboxes\n",
        "# 70\n",
        "found_bounding_boxes[:,2] = found_bounding_boxes[:,0] + found_bounding_boxes[:,2]\n",
        "found_bounding_boxes[:,3] = found_bounding_boxes[:,1] + found_bounding_boxes[:,3]\n",
        "boxIndices = non_max_suppression(found_bounding_boxes, weights.ravel(), threshold=0.2)\n",
        "found_bounding_boxes = found_bounding_boxes[boxIndices,:]\n",
        "found_bounding_boxes[:,2] = found_bounding_boxes[:,2] - found_bounding_boxes[:,0]\n",
        "found_bounding_boxes[:,3] = found_bounding_boxes[:,3] - found_bounding_boxes[:,1]\n",
        "print(len(found_bounding_boxes)) # number of boundingboxes\n",
        "# 4\n",
        "# copy the original image to draw bounding boxes on it for now, as we'll use it again later\n",
        "img_with_waw_bboxes = img.copy()\n",
        "for (hx, hy, hw, hh) in found_bounding_boxes:\n",
        "    cv2.rectangle(img_with_waw_bboxes, (hx, hy), (hx + hw, hy + hh), (0, 0, 255), 1)\n",
        "plt.figure(figsize=(20, 12))\n",
        "img_with_waw_bboxes = cv2.cvtColor(img_with_waw_bboxes, cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(img_with_waw_bboxes, aspect='auto'), plt.axis('off')\n",
        "plt.title('Boundingboxes found by HOG-SVM after non-max-suppression', size=20)\n",
        "plt.show()\n",
        "\n",
        "# with meanshiftgrouping to get rid of multiple detections of the same object\n",
        "(found_bounding_boxes, weights) = hog.detectMultiScale(img, winStride=(4, 4), padding=(8, 8), scale=1.01, useMeanshiftGrouping=True)\n",
        "print(len(found_bounding_boxes)) # number of boundingboxes\n",
        "# 3\n",
        "# copy the original image to draw bounding boxes on it for now, as we'll use it again later\n",
        "img_with_waw_bboxes = img.copy()\n",
        "for (hx, hy, hw, hh) in found_bounding_boxes:\n",
        "    cv2.rectangle(img_with_waw_bboxes, (hx, hy), (hx + hw, hy + hh), (0, 0, 255), 1)\n",
        "plt.figure(figsize=(20, 12))\n",
        "img_with_waw_bboxes = cv2.cvtColor(img_with_waw_bboxes, cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(img_with_waw_bboxes, aspect='auto'), plt.axis('off')\n",
        "plt.title('Boundingboxes found by HOG-SVM with meanshift grouping', size=20)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVyHyD-a1em4",
        "colab_type": "text"
      },
      "source": [
        "### Object Detection with Yolo V3 (OpenCV-python)\n",
        "Very long time of downloading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_No-CINo3hOY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://pjreddie.com/media/files/yolov3.weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5vW0QgM1em4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://pjreddie.com/darknet/yolo/\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import colorsys\n",
        "from random import shuffle\n",
        "\n",
        "# Initialize the parameters\n",
        "conf_threshold = 0.5  #Confidence threshold\n",
        "nms_threshold = 0.4   #Non-maximum suppression threshold\n",
        "width = 416       #Width of network's input image\n",
        "height = 416      #Height of network's input image\n",
        "\n",
        "# Load names of classes\n",
        "classes_file = \"models/yolov3/coco_classes.txt\";\n",
        "classes = None\n",
        "with open(classes_file, 'rt') as f:\n",
        "    classes = f.read().rstrip('\\n').split('\\n')\n",
        "\n",
        "# Give the configuration and weight files for the model and load the network using them.\n",
        "model_configuration = \"models/yolov3/yolov3.cfg\"\n",
        "model_weights = \"yolov3.weights\"\n",
        "\n",
        "net = cv2.dnn.readNetFromDarknet(model_configuration, model_weights)\n",
        "net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
        "net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
        "\n",
        "# Get the names of the output layers\n",
        "def get_output_layers(net):\n",
        "    # Get the names of all the layers in the network\n",
        "    layersNames = net.getLayerNames()\n",
        "    # Get the names of the output layers, i.e. the layers with unconnected outputs\n",
        "    return [layersNames[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
        "\n",
        "# Draw the predicted bounding box\n",
        "def draw_boxes(img, class_id, conf, left, top, right, bottom):\n",
        "    # Draw a bounding box.\n",
        "    label = \"{}: {:.2f}%\".format(classes[class_id], conf * 100)\n",
        "    color = tuple([int(255*x) for x in colors[class_id]])\n",
        "    top = top - 15 if top - 15 > 15 else top + 15\n",
        "    pil_im = Image.fromarray(cv2.cvtColor(img,cv2.COLOR_BGR2RGB)) \n",
        "    thickness = (img.shape[0] + img.shape[1]) // 300\n",
        "    font = ImageFont.truetype(\"images/verdana.ttf\", 25) \n",
        "    draw = ImageDraw.Draw(pil_im)  \n",
        "    label_size = draw.textsize(label, font)\n",
        "    if top - label_size[1] >= 0:\n",
        "        text_origin = np.array([left, top - label_size[1]])\n",
        "    else:\n",
        "        text_origin = np.array([left, top + 1])\n",
        "    for i in range(thickness):\n",
        "        draw.rectangle([left + i, top + i, right - i, bottom - i], outline=color)\n",
        "    draw.rectangle([tuple(text_origin), tuple(text_origin +  label_size)], fill=color)\n",
        "    draw.text(text_origin, label, fill=(0, 0, 0), font=font)\n",
        "    del draw\n",
        "    img = cv2.cvtColor(np.array(pil_im), cv2.COLOR_RGB2BGR)  \n",
        "    \n",
        "    return img\n",
        "\n",
        "\n",
        "# Remove the bounding boxes with low confidence using non-maxima suppression\n",
        "def post_process(img, outs):\n",
        "    heighteight = img.shape[0]\n",
        "    widthidth = img.shape[1]\n",
        "\n",
        "    class_ids = []\n",
        "    confidences = []\n",
        "    boxes = []\n",
        "    # Scan through all the bounding boxes output from the network and keep only the\n",
        "    # ones with high confidence scores. Assign the box's class label as the class with the highest score.\n",
        "    class_ids = []\n",
        "    confidences = []\n",
        "    boxes = []\n",
        "    for out in outs:\n",
        "        for detection in out:\n",
        "            scores = detection[5:]\n",
        "            class_id = np.argmax(scores)\n",
        "            confidence = scores[class_id]\n",
        "            if confidence > conf_threshold:\n",
        "                center_x = int(detection[0] * widthidth)\n",
        "                center_y = int(detection[1] * heighteight)\n",
        "                width = int(detection[2] * widthidth)\n",
        "                height = int(detection[3] * heighteight)\n",
        "                left = int(center_x - width / 2)\n",
        "                top = int(center_y - height / 2)\n",
        "                class_ids.append(class_id)\n",
        "                confidences.append(float(confidence))\n",
        "                boxes.append([left, top, width, height])\n",
        "\n",
        "    # Perform non maximum suppression to eliminate redundant overlapping boxes with\n",
        "    # lower confidences.\n",
        "    indices = cv2.dnn.NMSBoxes(boxes, confidences, conf_threshold, nms_threshold)\n",
        "    for i in indices:\n",
        "        i = i[0]\n",
        "        box = boxes[i]\n",
        "        left = box[0]\n",
        "        top = box[1]\n",
        "        width = box[2]\n",
        "        height = box[3]\n",
        "        img = draw_boxes(img, class_ids[i], confidences[i], left, top, left + width, top + height)\n",
        "        \n",
        "    return img\n",
        "\n",
        "hsv_tuples = [(x/len(classes), x/len(classes), 0.8) for x in range(len(classes))]\n",
        "shuffle(hsv_tuples)\n",
        "colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n",
        "\n",
        "img = cv2.imread('images/mytable.png')\n",
        "\n",
        "orig = np.copy(img)\n",
        "# Create a 4D blob from a img.\n",
        "blob = cv2.dnn.blobFromImage(img, 1/255, (width, height), [0,0,0], 1, crop=False)\n",
        "\n",
        "# Sets the input to the network\n",
        "net.setInput(blob)\n",
        "\n",
        "# Runs the forward pass to get output of the output layers\n",
        "outs = net.forward(get_output_layers(net))\n",
        "\n",
        "# Remove the bounding boxes with low confidence\n",
        "img = post_process(img, outs)\n",
        "\n",
        "fig = plt.figure(figsize=(20,15))\n",
        "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)), plt.axis('off'), plt.title('Objects detected with Yolo (v3)', size=20)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrYAbJhG1em7",
        "colab_type": "text"
      },
      "source": [
        "### Object Detection with Faster-RCNN (TensorFlow ResNet)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezseipvT6lWr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "https://github.com/tensorflow/models/tree/master/research/object_detection"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykxRvQN17qW6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://download.tensorflow.org/models/object_detection/faster_rcnn_resnet101_coco_2018_01_28.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAwsjI5I8G45",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar -xzvf faster_rcnn_resnet101_coco_2018_01_28.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-Q-NubJ9c74",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://github.com/JotJunior/PHP-Boleto-ZF2/raw/master/public/assets/fonts/arial.ttf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "MN2O_Bps1em7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "from PIL import Image, ImageFont, ImageDraw\n",
        "import json    \n",
        "import colorsys\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "# initialize the list of class labels MobileNet SSD was trained to\n",
        "# detect, then generate a set of bounding box colors for each class\n",
        "with open('models/faster_rcnn/image_info_test2017.json','r') as r:\n",
        "    js = json.loads(r.read())\n",
        "#js.keys()\n",
        "labels = {i['id']:i['name'] for i in js['categories']}\n",
        "print(labels)\n",
        "print(len(labels))\n",
        "\n",
        "hsv_tuples = [(x/len(labels), 0.8, 0.8) for x in range(len(labels))]\n",
        "colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n",
        "conf = 0.2\n",
        "\n",
        "# Read and preprocess an image.\n",
        "img = cv2.imread('images/road.png')\n",
        "\n",
        "# Read the graph.\n",
        "with tf.io.gfile.GFile('faster_rcnn_resnet101_coco_2018_01_28/frozen_inference_graph.pb', 'rb') as f:\n",
        "    graph_def = tf.compat.v1.GraphDef() #tf.GraphDef()\n",
        "    graph_def.ParseFromString(f.read())\n",
        "\n",
        "with tf.compat.v1.Session() as sess: #tf.Session() as sess:\n",
        "    # Restore session\n",
        "    sess.graph.as_default()\n",
        "    tf.import_graph_def(graph_def, name='')\n",
        "\n",
        "    orig = np.copy(img)\n",
        "\n",
        "    rows = img.shape[0]\n",
        "    cols = img.shape[1]\n",
        "    inp = cv2.resize(img, (300, 300))\n",
        "    inp = inp[:, :, [2, 1, 0]]  # BGR2RGB\n",
        "\n",
        "    # Run the model\n",
        "    out = sess.run([sess.graph.get_tensor_by_name('num_detections:0'),\n",
        "                    sess.graph.get_tensor_by_name('detection_scores:0'),\n",
        "                    sess.graph.get_tensor_by_name('detection_boxes:0'),\n",
        "                    sess.graph.get_tensor_by_name('detection_classes:0')],\n",
        "                   feed_dict={'image_tensor:0': inp.reshape(1, inp.shape[0], inp.shape[1], 3)})\n",
        "\n",
        "\n",
        "    #print(len(out))\n",
        "    # Visualize detected bounding boxes.\n",
        "    num_detections = int(out[0][0])\n",
        "    print(num_detections)\n",
        "    #print(out[1].shape) # prob\n",
        "    #print(out[2].shape) # bounding box\n",
        "    #print(out[3].shape) # class_id\n",
        "\n",
        "    for i in range(num_detections):\n",
        "        idx = int(out[3][0][i])\n",
        "        #print(class_id)\n",
        "        score = float(out[1][0][i])\n",
        "        bbox = [float(v) for v in out[2][0][i]]\n",
        "        if score > conf:\n",
        "            x = bbox[1] * cols\n",
        "            y = bbox[0] * rows\n",
        "            right = bbox[3] * cols\n",
        "            bottom = bbox[2] * rows\n",
        "\n",
        "        # draw the prediction on the image\n",
        "        label = \"{}: {:.2f}%\".format(labels[idx], score * 100)\n",
        "        color = tuple([int(255*x) for x in colors[idx]])\n",
        "        y = y - 15 if y - 15 > 15 else y + 15\n",
        "        pil_im = Image.fromarray(cv2.cvtColor(img,cv2.COLOR_BGR2RGB)) \n",
        "        thickness = (img.shape[0] + img.shape[1]) // 300\n",
        "        font = ImageFont.truetype(\"arial.ttf\", 15) \n",
        "        draw = ImageDraw.Draw(pil_im)  \n",
        "        label_size = draw.textsize(label, font)\n",
        "        if y - label_size[1] >= 0:\n",
        "            text_origin = np.array([x, y - label_size[1]])\n",
        "        else:\n",
        "            text_origin = np.array([x, y + 1])\n",
        "        for i in range(thickness):\n",
        "            draw.rectangle([x + i, y + i, right - i, bottom - i], outline=color)\n",
        "        draw.rectangle([tuple(text_origin), tuple(text_origin +  label_size)], fill=color)\n",
        "        draw.text(text_origin, label, fill=(0, 0, 0), font=font)\n",
        "        del draw\n",
        "        img = cv2.cvtColor(np.array(pil_im), cv2.COLOR_RGB2BGR)      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0r2E4Xmp1em-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize=(20,20))\n",
        "plt.imshow(cv2.cvtColor(np.array(img), cv2.COLOR_BGR2RGB)), plt.axis('off'), plt.title('Objects detected with Faster-RCNN', size=25)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFLzrppj1enA",
        "colab_type": "text"
      },
      "source": [
        "### Object Detection with Mask-RCNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvS-Mv2OHDuY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://download.tensorflow.org/models/object_detection/mask_rcnn_inception_v2_coco_2018_01_28.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhv95B9GHVXW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar -xzvf mask_rcnn_inception_v2_coco_2018_01_28.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPDXXp751enB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# http://download.tensorflow.org/models/object_detection/mask_rcnn_inception_v2_coco_2018_01_28.tar.gz\n",
        "# https://github.com/opencv2/opencv2_extra/blob/master/testdata/dnn/download_models.py\n",
        "# https://github.com/opencv2/opencv2_extra/tree/master/testdata/dnn\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os.path\n",
        "import sys\n",
        "import random\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "print(cv2.__version__)\n",
        "\n",
        "# Initialize the parameters\n",
        "conf_threshold = 0.5  # Confidence threshold\n",
        "mask_threshold = 0.3  # Mask threshold\n",
        "\n",
        "# Draw the predicted bounding box, colorize and show the mask on the image\n",
        "def draw_box(img, class_id, conf, left, top, right, bottom, class_mask):\n",
        "    # Draw a bounding box.\n",
        "    cv2.rectangle(img, (left, top), (right, bottom), (255, 178, 50), 3)\n",
        "    \n",
        "    # Print a label of class.\n",
        "    label = '%.2f' % conf\n",
        "    if classes:\n",
        "        assert(class_id < len(classes))\n",
        "        label = '%s:%s' % (classes[class_id], label)\n",
        "    \n",
        "    # Display the label at the top of the bounding box\n",
        "    label_size, baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n",
        "    top = max(top, label_size[1])\n",
        "    cv2.rectangle(img, (left, top - round(1.5*label_size[1])), (left + round(1.5*label_size[0]), top + baseline), (255, 255, 255), cv2.FILLED)\n",
        "    cv2.putText(img, label, (left, top), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0,0,0), 1)\n",
        "\n",
        "    # Resize the mask, threshold, color and apply it on the image\n",
        "    class_mask = cv2.resize(class_mask, (right - left + 1, bottom - top + 1))\n",
        "    mask = (class_mask > mask_threshold)\n",
        "    roi = img[top:bottom+1, left:right+1][mask]\n",
        "\n",
        "    # color = colors[class_id%len(colors)]\n",
        "    # Comment the above line and uncomment the two lines below to generate different instance colors\n",
        "    color_index = random.randint(0, len(colors)-1)\n",
        "    color = colors[color_index]\n",
        "\n",
        "    img[top:bottom+1, left:right+1][mask] = ([0.3*color[0], 0.3*color[1], 0.3*color[2]] + 0.7 * roi).astype(np.uint8)\n",
        "\n",
        "    # Draw the contours on the image\n",
        "    mask = mask.astype(np.uint8)\n",
        "    #im2, \n",
        "    contours, hierarchy = cv2.findContours(mask,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
        "    cv2.drawContours(img[top:bottom+1, left:right+1], contours, -1, color, 3, cv2.LINE_8, hierarchy, 100)\n",
        "\n",
        "# For each img, extract the bounding box and mask for each detected object\n",
        "def post_process(boxes, masks):\n",
        "    # Output size of masks is NxCxHxW where\n",
        "    # N - number of detected boxes\n",
        "    # C - number of classes (excluding background)\n",
        "    # HxW - segmentation shape\n",
        "    num_classes = masks.shape[1]\n",
        "    num_detections = boxes.shape[2]\n",
        "\n",
        "    height = img.shape[0]\n",
        "    width = img.shape[1]\n",
        "\n",
        "    for i in range(num_detections):\n",
        "        box = boxes[0, 0, i]\n",
        "        mask = masks[i]\n",
        "        score = box[2]\n",
        "        if score > conf_threshold:\n",
        "            class_id = int(box[1])\n",
        "            \n",
        "            # Extract the bounding box\n",
        "            left = int(width * box[3])\n",
        "            top = int(height * box[4])\n",
        "            right = int(width * box[5])\n",
        "            bottom = int(height * box[6])\n",
        "            \n",
        "            left = max(0, min(left, width - 1))\n",
        "            top = max(0, min(top, height - 1))\n",
        "            right = max(0, min(right, width - 1))\n",
        "            bottom = max(0, min(bottom, height - 1))\n",
        "            \n",
        "            # Extract the mask for the object\n",
        "            class_mask = mask[class_id]\n",
        "\n",
        "            # Draw bounding box, colorize and show the mask on the image\n",
        "            draw_box(img, class_id, score, left, top, right, bottom, class_mask)\n",
        "\n",
        "\n",
        "# Load names of classes\n",
        "classesFile = \"models/mask_rcnn/mscoco_labels.names\";\n",
        "classes = None\n",
        "with open(classesFile, 'rt') as f:\n",
        "   classes = f.read().rstrip('\\n').split('\\n')\n",
        "\n",
        "# Give the textGraph and weight files for the model\n",
        "textGraph = \"models/mask_rcnn/mask_rcnn_inception_v2_coco_2018_01_28.pbtxt\";\n",
        "model_weights = \"mask_rcnn_inception_v2_coco_2018_01_28/frozen_inference_graph.pb\";\n",
        "\n",
        "# Load the network\n",
        "net = cv2.dnn.readNetFromTensorflow(model_weights, textGraph);\n",
        "net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
        "net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
        "\n",
        "# Load the classes\n",
        "colors_file = \"models/mask_rcnn/colors.txt\";\n",
        "with open(colors_file, 'rt') as f:\n",
        "    colors_str = f.read().rstrip('\\n').split('\\n')\n",
        "colors = [] #[0,0,0]\n",
        "for i in range(len(colors_str)):\n",
        "    rgb = colors_str[i].split(' ')\n",
        "    color = np.array([float(rgb[0]), float(rgb[1]), float(rgb[2])])\n",
        "    colors.append(color)\n",
        "\n",
        "img = cv2.imread('images/road.png')\n",
        "    \n",
        "print(img.shape)\n",
        "\n",
        "orig = np.copy(img)\n",
        "#cv2.imwrite('Mask-RCNN/input/img_' + str(i).zfill(4) + '.jpg', orig)\n",
        "\n",
        "# Create a 4D blob from a img.\n",
        "blob = cv2.dnn.blobFromImage(img, swapRB=True, crop=False)\n",
        "\n",
        "# Set the input to the network\n",
        "net.setInput(blob)\n",
        "\n",
        "# Run the forward pass to get output from the output layers\n",
        "boxes, masks = net.forward(['detection_out_final', 'detection_masks'])\n",
        "\n",
        "# Extract the bounding box and mask for each of the detected objects\n",
        "post_process(boxes, masks)\n",
        "\n",
        "# Put efficiency information.\n",
        "t, _ = net.getPerfProfile()\n",
        "#label = 'Mask-RCNN on 2.5 GHz Intel Core i7 CPU, Inference time for a img : %0.0f ms' % abs(t * 1000.0 / cv2.getTickFrequency())\n",
        "#cv2.putText(img, label, (0, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0))\n",
        "\n",
        "fig = plt.figure(figsize=(20,20))\n",
        "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05) \n",
        "plt.subplot(211), plt.imshow(cv2.cvtColor(orig, cv2.COLOR_BGR2RGB)), plt.axis('off'), plt.title('Original Image', size=20)\n",
        "plt.subplot(212), plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)), plt.axis('off'), plt.title('Objects detected with Mask-RCNN', size=20)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QqYpTx-1enD",
        "colab_type": "text"
      },
      "source": [
        "### Text Detection in images with Tesseract"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46zovVJiKz4i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!sudo apt install tesseract-ocr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCqlCbHLIuZu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pytesseract"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpCVEBxoJUzK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://codeload.github.com/ZER-0-NE/EAST-Detector-for-text-detection-using-OpenCV/zip/master"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVFHMsrIJtQs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip master\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRbnni0I1enE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://stackoverflow.com/questions/44619077/pytesseract-ocr-multiple-config-options\n",
        "# https://codeload.github.com/ZER-0-NE/EAST-Detector-for-text-detection-using-OpenCV/zip/master\n",
        "# import the necessary packages\n",
        "from imutils.object_detection import non_max_suppression\n",
        "import numpy as np\n",
        "import pytesseract\n",
        "import cv2\n",
        "\n",
        "min_confidence = 0.5\n",
        "\n",
        "def decode_predictions(scores, geometry):\n",
        "    # grab the number of rows and columns from the scores volume, then\n",
        "    # initialize our set of bounding box rectangles and corresponding\n",
        "    # confidence scores\n",
        "    (num_rows, num_cols) = scores.shape[2:4]\n",
        "    rects = []\n",
        "    confidences = []\n",
        " \n",
        "    # loop over the number of rows\n",
        "    for y in range(0, num_rows):\n",
        "        # extract the scores (probabilities), followed by the\n",
        "        # geometrical data used to derive potential bounding box\n",
        "        # coordinates that surround text\n",
        "        scores_data = scores[0, 0, y]\n",
        "        x_data0 = geometry[0, 0, y]\n",
        "        x_data1 = geometry[0, 1, y]\n",
        "        x_data2 = geometry[0, 2, y]\n",
        "        x_data3 = geometry[0, 3, y]\n",
        "        angles_data = geometry[0, 4, y]\n",
        " \n",
        "        # loop over the number of columns\n",
        "        for x in range(0, num_cols):\n",
        "            # if our score does not have sufficient probability,\n",
        "            # ignore it\n",
        "            if scores_data[x] < min_confidence:\n",
        "                continue\n",
        " \n",
        "            # compute the offset factor as our resulting feature\n",
        "            # maps will be 4x smaller than the input image\n",
        "            (offset_x, offset_y) = (x * 4.0, y * 4.0)\n",
        " \n",
        "            # extract the rotation angle for the prediction and\n",
        "            # then compute the sin and cosine\n",
        "            angle = angles_data[x]\n",
        "            cos = np.cos(angle)\n",
        "            sin = np.sin(angle)\n",
        " \n",
        "            # use the geometry volume to derive the width and height\n",
        "            # of the bounding box\n",
        "            h = x_data0[x] + x_data2[x]\n",
        "            w = x_data1[x] + x_data3[x]\n",
        " \n",
        "            # compute both the starting and ending (x, y)-coordinates\n",
        "            # for the text prediction bounding box\n",
        "            end_x = int(offset_x + (cos * x_data1[x]) + (sin * x_data2[x]))\n",
        "            end_y = int(offset_y - (sin * x_data1[x]) + (cos * x_data2[x]))\n",
        "            start_x = int(end_x - w)\n",
        "            start_y = int(end_y - h)\n",
        " \n",
        "            # add the bounding box coordinates and probability score\n",
        "            # to our respective lists\n",
        "            rects.append((start_x, start_y, end_x, end_y))\n",
        "            confidences.append(scores_data[x])\n",
        " \n",
        "    # return a tuple of the bounding boxes and associated confidences\n",
        "    return (rects, confidences)\n",
        "\n",
        "# load the input image and grab the image dimensions\n",
        "im = 'images/book_cover.png'\n",
        "image = cv2.imread(im)\n",
        "orig = image.copy()\n",
        "(origH, origW) = image.shape[:2]\n",
        " \n",
        "# set the new width and height and then determine the ratio in change\n",
        "# for both the width and height\n",
        "width = height = 32*10 #320\n",
        "(w, h) = (width, height)\n",
        "rW = origW / float(w)\n",
        "rH = origH / float(h)\n",
        " \n",
        "# resize the image and grab the new image dimensions\n",
        "image = cv2.resize(image, (w, h))\n",
        "(H, W) = image.shape[:2]\n",
        "\n",
        "# define the two output layer names for the EAST detector model that\n",
        "# we are interested in -- the first is the output probabilities and the\n",
        "# second can be used to derive the bounding box coordinates of text\n",
        "layerNames = [\n",
        "    \"feature_fusion/Conv_7/Sigmoid\",\n",
        "    \"feature_fusion/concat_3\"]\n",
        " \n",
        "# load the pre-trained EAST text detector\n",
        "print(\"loading EAST text detector...\")\n",
        "net = cv2.dnn.readNet('EAST-Detector-for-text-detection-using-OpenCV-master/frozen_east_text_detection.pb')\n",
        "\n",
        "# construct a blob from the image and then perform a forward pass of\n",
        "# the model to obtain the two output layer sets\n",
        "b, g, r = np.mean(image[...,0]), np.mean(image[...,1]), np.mean(image[...,2])\n",
        "blob = cv2.dnn.blobFromImage(image, 1.0, (W, H), (b, g, r), swapRB=True, crop=False)\n",
        "net.setInput(blob)\n",
        "(scores, geometry) = net.forward(layerNames)\n",
        " \n",
        "# decode the predictions, then  apply non-maxima suppression to\n",
        "# suppress weak, overlapping bounding boxes\n",
        "(rects, confidences) = decode_predictions(scores, geometry)\n",
        "boxes = non_max_suppression(np.array(rects), probs=confidences)\n",
        "\n",
        "padding = 0.001 #0.01 #0.5\n",
        "# initialize the list of results\n",
        "results = []\n",
        " \n",
        "# loop over the bounding boxes\n",
        "for (start_x, start_y, end_x, end_y) in boxes:\n",
        "    # scale the bounding box coordinates based on the respective ratios\n",
        "    start_x = int(start_x * rW)\n",
        "    start_y = int(start_y * rH)\n",
        "    end_x = int(end_x * rW)\n",
        "    end_y = int(end_y * rH)\n",
        "\n",
        "    # in order to obtain a better OCR of the text we can potentially\n",
        "    # apply a bit of padding surrounding the bounding box -- here we\n",
        "    # are computing the deltas in both the x and y directions\n",
        "    dX = int((end_x - start_x) * padding)\n",
        "    dY = int((end_y - start_y) * padding)\n",
        "\n",
        "    # apply padding to each side of the bounding box, respectively\n",
        "    start_x = max(0, start_x - dX*2)\n",
        "    start_y = max(0, start_y - dY*2)\n",
        "    end_x = min(origW, end_x + (dX * 2))\n",
        "    end_y = min(origH, end_y + (dY * 2))\n",
        "\n",
        "    # extract the actual padded ROI\n",
        "    roi = orig[start_y:end_y, start_x:end_x]\n",
        "\n",
        "    # in order to apply Tesseract v4 to OCR text we must supply\n",
        "    # (1) a language, (2) an OEM flag of 4, indicating that the we\n",
        "    # wish to use the LSTM neural net model for OCR, and finally\n",
        "    # (3) an OEM value, in this case, 7 which implies that we are\n",
        "    # treating the ROI as a single line of text\n",
        "    config = (\"-l eng --oem 1 --psm 11\")\n",
        "    text = pytesseract.image_to_string(roi, config=config)\n",
        "    #print(text)\n",
        "\n",
        "    # add the bounding box coordinates and OCR'd text to the list\n",
        "    # of results\n",
        "    results.append(((start_x, start_y, end_x, end_y), text))\n",
        "\n",
        "    # sort the results bounding box coordinates from top to bottom\n",
        "    results = sorted(results, key=lambda r:r[0][1])\n",
        " \n",
        "print(len(results))\n",
        "# loop over the results\n",
        "output = orig.copy()\n",
        "i = 1\n",
        "for ((start_x, start_y, end_x, end_y), text) in results:\n",
        "    # display the text OCR'd by Tesseract\n",
        "    #print(\"OCR TEXT\")\n",
        "    #print(\"========\")\n",
        "    print(text)\n",
        "\n",
        "    # strip out non-ASCII text so we can draw the text on the image\n",
        "    # using OpenCV, then draw the text and a bounding box surrounding\n",
        "    # the text region of the input image\n",
        "    text = \"\".join([c if ord(c) < 128 else \"\" for c in text]).strip()\n",
        "    cv2.rectangle(output, (start_x, start_y), (end_x, end_y), (0, 255, 0), 2)\n",
        "    cv2.putText(output, text, (start_x, start_y - 20), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 3)\n",
        "\n",
        "    # show the output image\n",
        "    i += 1\n",
        "cv2.imwrite(\"images/text_\" + im.split('/')[-1], output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTAzZ3gA1enH",
        "colab_type": "text"
      },
      "source": [
        "![](images\\text_book_cover.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySvy9gKt1enH",
        "colab_type": "text"
      },
      "source": [
        "### Multiple Object Tracking with opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JDZe7A5MWKJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip uninstall opecv-python \n",
        "!pip uninstall opencv-contrib-python\n",
        "!pip install opencv-python==3.4.4.19\n",
        "!pip install opencv-contrib-python==3.4.4.19"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VP_tTAC-1enI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://www.youtube.com/watch?v=whXnYIgT4P0\n",
        "# https://stackoverflow.com/questions/54013403/attribute-error-multitracker-create-not-found-in-cv2-on-raspberry-pi\n",
        "#print(cv2.getBuildInformation())\n",
        "#pip uninstall opecv-python \n",
        "#pip uninstall opencv-contrib-python\n",
        "#pip install opencv-python==3.4.4.19\n",
        "#pip install opencv-contrib-python==3.4.4.19\n",
        "import time\n",
        "import cv2\n",
        "import matplotlib.pylab as plt\n",
        "from imutils import resize\n",
        "\n",
        "print(cv2.__version__)\n",
        "# 3.4.4\n",
        "\n",
        "# Create MultiTracker object\n",
        "multi_tracker = cv2.MultiTracker_create()\n",
        " \n",
        "# initialize the bounding box coordinates of the object (car) we are going to track\n",
        "car_bbox = (141,175,45,29) \n",
        "car2_bbox = (295,170,55,39) \n",
        "bboxes = [car_bbox, car2_bbox]\n",
        "colors = [(0, 255, 255), (255, 255, 0)]\n",
        "\n",
        "vs = cv2.VideoCapture('images/road.mp4')\n",
        "\n",
        "_, frame = vs.read()\n",
        "frame = resize(frame, width=500)\n",
        "print(frame.shape)\n",
        "\n",
        "# start OpenCV object tracker using the supplied bounding box\n",
        "# coordinates, then start the FPS throughput estimator as well\n",
        "# tracker.init(frame, car_bbox)\n",
        "\n",
        "for bbox in bboxes:\n",
        "    multi_tracker.add(cv2.TrackerCSRT_create(), frame, bbox)\n",
        "\n",
        "j = 0\n",
        "fig = plt.figure(figsize=(20,55))\n",
        "# loop over frames from the video stream\n",
        "while True:\n",
        "    # grab the current frame, then handle if we are using a\n",
        "    # VideoStream or VideoCapture object\n",
        "    vs.set(cv2.CAP_PROP_POS_MSEC,(j*300))    # added this line - 1 sec 3 frames\n",
        "    _, frame = vs.read()\n",
        "    \n",
        "    # check to see if we have reached the end of the stream\n",
        "    if frame is None or j//4 > 18:\n",
        "        break\n",
        "\n",
        "    # resize the frame (so we can process it faster) and grab the\n",
        "    # frame dimensions\n",
        "    frame = resize(frame, width=500)\n",
        "\n",
        "    (H, W) = frame.shape[:2]\n",
        "    # grab the new bounding box coordinates of the object\n",
        "    #(success, box) = tracker.update(frame)\n",
        "\n",
        "    # get updated location of objects in subsequent frames\n",
        "    success, boxes = multi_tracker.update(frame)\n",
        " \n",
        "    # check to see if the tracking was a success\n",
        "    if success:\n",
        "        # draw tracked objects\n",
        "        for i, box in enumerate(boxes):\n",
        "            p1 = (int(box[0]), int(box[1]))\n",
        "            p2 = (int(box[0] + box[2]), int(box[1] + box[3]))\n",
        "            cv2.rectangle(frame, p1, p2, colors[i], 2, 1)\n",
        "\n",
        "    # show the output frame\n",
        "    if j%4 == 0:\n",
        "        plt.subplot(9,2,j//4+1)\n",
        "        plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)), plt.axis('off'), plt.title('Frame {}'.format(j), size=20)\n",
        "\n",
        "    j += 1\n",
        "\n",
        "plt.suptitle('Tracking cars in a video with CSRT MultiTracker', size=30)\n",
        "fig.subplots_adjust(left=0, right=1, bottom=0, top=0.95, hspace=0.05, wspace=0.05) \n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rW7xNM3Q1enK",
        "colab_type": "text"
      },
      "source": [
        "### Face Detection with Viola Jones / Haar-like features and Adaboost Cascade"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rz_oj5BnNK3q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip uninstall opecv-python \n",
        "!pip uninstall opencv-contrib-python\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWJqVaiHNQfI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install opencv-python\n",
        "!pip install opencv-contrib-python"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y28e0WcJ1enK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://github.com/opencv/opencv/blob/master/data/haarcascades/haarcascade_smile.xml\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "face_cascade = cv2.CascadeClassifier('models/face_detect/haarcascade_frontalface_alt2.xml')\n",
        "eye_cascade = cv2.CascadeClassifier('models/face_detect/haarcascade_eye.xml') # haarcascade_eye_tree_eyeglasses.xml\n",
        "smile_cascade = cv2.CascadeClassifier('models/face_detect/haarcascade_smile.xml')\n",
        "\n",
        "img = cv2.imread('images/all.png')\n",
        "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "faces = face_cascade.detectMultiScale(gray, 1.01, 8) # scaleFactor=1.2, minNbr=5\n",
        "print(len(faces)) # number of faces detected\n",
        "for (x,y,w,h) in faces:\n",
        "    img = cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
        "    roi_gray = gray[y:y+h, x:x+w]\n",
        "    roi_color = img[y:y+h, x:x+w]\n",
        "    eyes = eye_cascade.detectMultiScale(roi_gray, 1.04, 10)\n",
        "    #print(eyes) # location of eyes detected\n",
        "    for (ex,ey,ew,eh) in eyes:\n",
        "        cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\n",
        "    smile = smile_cascade.detectMultiScale(roi_gray, 1.38, 6)\n",
        "    for (mx,my,mw,mh) in smile:\n",
        "        cv2.rectangle(roi_color,(mx,my),(mx+mw,my+mh),(0,0,255),2)\n",
        "       \n",
        "plt.figure(figsize=(15,20))\n",
        "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)), plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BkXMQ6x1enM",
        "colab_type": "text"
      },
      "source": [
        "### Face Detection with dlib using HOG features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsduGLXj1enN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import dlib\n",
        "img = cv2.cvtColor(cv2.imread('images/all.png'), cv2.COLOR_BGR2RGB)\n",
        "hog_detector = dlib.get_frontal_face_detector()\n",
        "faces = hog_detector(img, 0)\n",
        "print(len(faces)) # number of faces detected\n",
        "for face in faces:\n",
        "    l,t,r,b = face.left(), face.top(), face.right(), face.bottom()\n",
        "    img = cv2.rectangle(img,(l,t),(r,b),(0,0,255),2)\n",
        "plt.figure(figsize=(15,20))\n",
        "plt.imshow(img), plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcB5Tl4F1enP",
        "colab_type": "text"
      },
      "source": [
        "### Face Detection with opencv-python Single Shot MultiBox Detector pretrained deeplearning model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlWPznWUOadw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://github.com/opencv/opencv_3rdparty/raw/8033c2bc31b3256f0d461c919ecc01c2428ca03b/opencv_face_detector_uint8.pb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fifXYGJq1enQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://github.com/opencv/opencv_extra/blob/master/testdata/dnn/opencv_face_detector.pbtxt\n",
        "# https://github.com/opencv/opencv_3rdparty/raw/8033c2bc31b3256f0d461c919ecc01c2428ca03b/opencv_face_detector_uint8.pb\n",
        "import cv2\n",
        "\n",
        "tf_model = \"opencv_face_detector_uint8.pb\"\n",
        "tf_config = \"models/face_detect/opencv_face_detector.pbtxt\"\n",
        "net = cv2.dnn.readNetFromTensorflow(tf_model, tf_config)\n",
        "\n",
        "img = cv2.imread('images/beatles.png')\n",
        "h, w = img.shape[:2]\n",
        "print(h,w)\n",
        "blob = cv2.dnn.blobFromImage(img, 1.0, (w, h), [104, 117, 123], False, False)\n",
        "net.setInput(blob)\n",
        "detections = net.forward()\n",
        "print(detections.shape)\n",
        "bboxes = []\n",
        "threshold = 0.5\n",
        "for i in range(detections.shape[2]):\n",
        "    confidence = detections[0, 0, i, 2]\n",
        "    if confidence > threshold:\n",
        "        print(confidence)\n",
        "        l = int(detections[0, 0, i, 3] * w)\n",
        "        t = int(detections[0, 0, i, 4] * h)\n",
        "        r = int(detections[0, 0, i, 5] * w)\n",
        "        b = int(detections[0, 0, i, 6] * h)\n",
        "        img = cv2.rectangle(img,(l,t),(r,b),(0,0,255),2)\n",
        "plt.figure(figsize=(15,20))\n",
        "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)), plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSrFuRus1enS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}