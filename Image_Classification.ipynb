{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Image Classification",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mawhy/OpenCV/blob/master/Image_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-5gCHfmEyyB",
        "colab_type": "text"
      },
      "source": [
        "# Image Processing CookBook\n",
        "## Image_Classification\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhHdO-rs6NLP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/PacktPublishing/Python-Image-Processing-Cookbook.git\n",
        "%cp -av \"/content/Python-Image-Processing-Cookbook/Chapter 07/images/\" \"/content/\"\n",
        "%cp -av \"/content/Python-Image-Processing-Cookbook/Chapter 07/models/\" \"/content/\"\n",
        "%rm -rf \"/content/Python-Image-Processing-Cookbook\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svKVB5PDEyyC",
        "colab_type": "text"
      },
      "source": [
        "### Image Classification with scikit-learn (HOG + Logistic Regression)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZFL3OJrEyyD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# http://www.vision.caltech.edu/Image_Datasets/Caltech101/101_ObjectCategories.tar.gz\n",
        "# http://www.vision.caltech.edu/Image_Datasets/Caltech101/\n",
        "# https://www.kaggle.com/manikg/training-svm-classifier-with-hog-features\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "from skimage.io import imread\n",
        "from skimage.color import gray2rgb\n",
        "from skimage.transform import resize\n",
        "from skimage.feature import hog\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report,accuracy_score\n",
        "from glob import glob\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "images, hog_images = [], []\n",
        "X, y = [], []\n",
        "ppc = 16\n",
        "sz = 200\n",
        "for dir in glob('images/Caltech101_images/*'):\n",
        "    image_files = glob(dir + '/*.jpg')\n",
        "    label = dir.split('\\\\')[-1]\n",
        "    print(label, len(image_files))\n",
        "    for image_file in image_files:\n",
        "        image = resize(imread(image_file), (sz,sz))\n",
        "        if len(image.shape) == 2: # if a gray-scale image\n",
        "            image = gray2rgb(image)\n",
        "        fd,hog_image = hog(image, orientations=8, pixels_per_cell=(ppc,ppc),cells_per_block=(4, 4),block_norm= 'L2',visualize=True, multichannel=True)\n",
        "        images.append(image)\n",
        "        hog_images.append(hog_image)\n",
        "        X.append(fd)\n",
        "        y.append(label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FECPiC5MEyyH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(images), hog_images[0].shape, X[0].shape, X[1].shape, len(y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sgvq4a3qEyyL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n = 6\n",
        "indices = np.random.choice(len(images), n*n)\n",
        "plt.figure(figsize=(20,20))\n",
        "plt.gray()\n",
        "i = 1\n",
        "for index in indices:\n",
        "    plt.subplot(n,n,i), plt.imshow(images[index]), plt.axis('off'), plt.title(y[index], size=20)\n",
        "    i += 1\n",
        "plt.show()\n",
        "plt.figure(figsize=(20,20))\n",
        "i = 1\n",
        "for index in indices:\n",
        "    plt.subplot(n,n,i), plt.imshow(hog_images[index]), plt.axis('off'), plt.title(y[index], size=20)\n",
        "    i += 1\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPwhMZ7IEyyQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "indices = np.arange(len(X))\n",
        "X_train, X_test, y_train, y_test, id_train, id_test = train_test_split(X, y, indices, test_size=0.1, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rA-jC2gEyyU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#clf = svm.LinearSVC(C=10)\n",
        "clf = LogisticRegression(C=1000, random_state=0, solver='lbfgs', multi_class='multinomial')\n",
        "clf.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hOkc2SXEyyX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(X.shape, y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5QVqNGbEyyZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = clf.predict(X_train)\n",
        "print(\"Accuracy: \" + str(accuracy_score(y_train, y_pred)))\n",
        "print('\\n')\n",
        "print(classification_report(y_train, y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShNeJKy1Eyyc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = clf.predict(X_test)\n",
        "print(\"Accuracy: \" + str(accuracy_score(y_test, y_pred)))\n",
        "print('\\n')\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "vWAZbk7AEyye",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(20,20))\n",
        "j = 0\n",
        "for i in id_test:\n",
        "    plt.subplot(10,10,j+1), plt.imshow(images[i]), plt.axis('off'), plt.title('{}/{}'.format(y_test[j], y_pred[j]))\n",
        "    j += 1\n",
        "plt.suptitle('Actual vs. Predicted Class Labels', size=20)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9iqFyP5Eyyh",
        "colab_type": "text"
      },
      "source": [
        "### Image Classification with VGG-19 / Inception V3 / MobileNet / ResNet101 (with deep learning, pytorch)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Thmw9XcpNTDa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://github.com/JotJunior/PHP-Boleto-ZF2/raw/master/public/assets/fonts/arial.ttf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLfY1N28Eyyh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "def classify(img, model_index, model_name, model_pred, labels):\n",
        "    #print(model_name, model_pred.shape)\n",
        "    _, index = torch.max(model_pred, 1)\n",
        "    model_pred, indices = torch.sort(model_pred, dim=1, descending=True)\n",
        "    percentage = torch.nn.functional.softmax(model_pred, dim=1)[0] * 100\n",
        "    print(labels[index[0]], percentage[0].item())\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    font = ImageFont.truetype(r'arial.ttf', 50)\n",
        "    draw.text((5, 5+model_index*50),'{}, pred: {},{}%'.format(model_name, labels[index[0]], round(percentage[0].item(),2)),(255,0,0),font=font)\n",
        "    return indices, percentage\n",
        "\n",
        "    \n",
        "#print(dir(models))\n",
        "\n",
        "with open('models/imagenet_classes.txt') as f:\n",
        "    labels = [line.strip() for line in f.readlines()]\n",
        "\n",
        "\n",
        "transform = transforms.Compose([            \n",
        " transforms.Resize(256),                    \n",
        " transforms.CenterCrop(224),                \n",
        " transforms.ToTensor(),                     \n",
        " transforms.Normalize(                      \n",
        " mean=[0.485, 0.456, 0.406],                \n",
        " std=[0.229, 0.224, 0.225]                  \n",
        " )])\n",
        "\n",
        "for imgfile in [\"images/cheetah.png\", \"images/swan.png\"]:\n",
        "    \n",
        "    img = Image.open(imgfile).convert('RGB')\n",
        "    img_t = transform(img)\n",
        "    batch_t = torch.unsqueeze(img_t, 0)\n",
        "\n",
        "    vgg19 = models.vgg19(pretrained=True)\n",
        "    vgg19.eval()\n",
        "    pred = vgg19(batch_t)\n",
        "    classify(img, 0, 'vgg19', pred, labels)\n",
        "\n",
        "    mobilenetv2 = models.mobilenet_v2(pretrained=True)\n",
        "    mobilenetv2.eval()\n",
        "    pred = mobilenetv2(batch_t)\n",
        "    classify(img, 1, 'mobilenetv2', pred, labels)\n",
        "\n",
        "    inceptionv3 = models.inception_v3(pretrained=True)\n",
        "    inceptionv3.eval()\n",
        "    pred = inceptionv3(batch_t)\n",
        "    classify(img, 2, 'inceptionv3', pred, labels)\n",
        "\n",
        "    resnet101 = models.resnet101(pretrained=True)\n",
        "    resnet101.eval()\n",
        "    pred = resnet101(batch_t)\n",
        "    indices, percentages = classify(img, 3, 'resnet101', pred, labels)\n",
        "    \n",
        "    plt.figure(figsize=(20,10))\n",
        "    plt.subplot(121), plt.imshow(img), plt.axis('off'), plt.title('image classified with pytorch', size=20)\n",
        "    plt.subplot(122), plt.bar(range(5), percentages.detach().numpy()[:5], align='center', alpha=0.5)\n",
        "    #print(indices[0].detach().numpy()[:5])\n",
        "    plt.xticks(range(5),  np.array(labels)[indices.detach().numpy().astype(int)[0][:5]])\n",
        "    plt.xlabel('predicted labels', size=20), plt.ylabel('predicted percentage', size=20)\n",
        "    plt.title('Resnet top 5 classes predicted', size=20)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fq9pIwpEyyj",
        "colab_type": "text"
      },
      "source": [
        "### Traffic Signal Classification with deep learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JxQ_uRqRqBs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB_Final_Training_Images.zip\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXXQ0sEcR__e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip GTSRB_Final_Training_Images.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnMDjyMUEyyk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!mkdir traffic_signs\n",
        "import os, glob\n",
        "from shutil import copy\n",
        "import pandas as pd\n",
        "\n",
        "image_dir = 'GTSRB/Final_Training/Images/'\n",
        "dest_dir = 'traffic_signs'\n",
        "df = pd.DataFrame()\n",
        "for d in sorted(os.listdir(image_dir)):\n",
        "    #print(d)\n",
        "    images = sorted(glob.glob(os.path.join(image_dir, d, '*.ppm')))\n",
        "    for img in images:\n",
        "        copy(img, dest_dir)\n",
        "    for csv in sorted(glob.glob(os.path.join(image_dir, d, '*.csv'))):\n",
        "        df1 = pd.read_csv(csv, sep=';')\n",
        "        df = df.append(df1)\n",
        "        #print(df.head())\n",
        "        print(d, len(images), df1.shape)\n",
        "df.to_csv(os.path.join(dest_dir, 'labels.csv'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9vyNptWEyyo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6b3LwkGEyys",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKwROBkFEyyv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(glob.glob(os.path.join(dest_dir, '*.ppm')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxjFZgPzEyyx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as  pd\n",
        "signal_names =  pd.read_csv('images/signal_names.csv')\n",
        "signal_names.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z26CAcH9lRGQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://bitbucket.org/jadslim/german-traffic-signs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ni_jCHmBEyyz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "import seaborn as sns\n",
        "\n",
        "training_file = \"german-traffic-signs/train.p\"\n",
        "validation_file = \"german-traffic-signs/valid.p\"\n",
        "testing_file = \"german-traffic-signs/test.p\"\n",
        "\n",
        "with open(training_file, mode='rb') as f:\n",
        "    train = pickle.load(f)\n",
        "with open(validation_file, mode='rb') as f:\n",
        "    valid = pickle.load(f)\n",
        "with open(testing_file, mode='rb') as f:\n",
        "    test = pickle.load(f)\n",
        "\n",
        "X_train, y_train = train['features'], train['labels']\n",
        "X_valid, y_valid = valid['features'], valid['labels']\n",
        "X_test, y_test = test['features'], test['labels']\n",
        "n_signs = len(np.unique(y_train))\n",
        "\n",
        "print(X_train.shape, X_valid.shape, X_test.shape, n_signs)\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "# plot barh chart with index as x values\n",
        "ax = sns.barplot(list(range(n_signs)), np.bincount(y_train))\n",
        "ax.get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}\".format(int(x))))\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(20, 20))\n",
        "for c in range(n_signs):\n",
        "    i = np.random.choice(np.where(y_train == c)[0])\n",
        "    plt.subplot(8, 6, c+1)\n",
        "    plt.axis('off')\n",
        "    plt.title(signal_names.loc[signal_names['ClassId'] == c].SignName.to_string(index=False))\n",
        "    plt.imshow(X_train[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5N2vN9e5UhT6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install livelossplot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw-lPuCPEyy2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2 \n",
        "import torch\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import make_grid\n",
        "import torch.utils.data.sampler as sampler\n",
        "from torch import nn, optim\n",
        "from livelossplot import PlotLosses\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "\n",
        "class TraffficNet(nn.Module):\n",
        "    def __init__(self, gray=False):\n",
        "        super(TraffficNet, self).__init__()\n",
        "        input_chan = 1 if gray else 3\n",
        "        self.conv1 = nn.Conv2d(input_chan, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 43)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "    \n",
        "class ClaheTranform:\n",
        "    def __init__(self, clipLimit=2.5, tileGridSize=(4, 4)):\n",
        "        self.clipLimit = clipLimit\n",
        "        self.tileGridSize = tileGridSize\n",
        "\n",
        "    def __call__(self, im):\n",
        "        img_y = cv2.cvtColor(im, cv2.COLOR_RGB2YCrCb)[:,:,0]\n",
        "        clahe = cv2.createCLAHE(clipLimit=self.clipLimit, tileGridSize=self.tileGridSize)\n",
        "        img_y = clahe.apply(img_y)\n",
        "        img_output = img_y.reshape(img_y.shape + (1,))\n",
        "        return img_output\n",
        "\n",
        "class PickledTrafficSignsDataset(Dataset):\n",
        "    def __init__(self, file_path, transform=None):\n",
        "        with open(file_path, mode='rb') as f:\n",
        "            data = pickle.load(f)\n",
        "            self.features = data['features']\n",
        "            self.labels = data['labels']\n",
        "            self.count = len(self.labels)\n",
        "            self.transform = transform\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        feature = self.features[index]\n",
        "        if self.transform is not None:\n",
        "            feature = self.transform(feature)\n",
        "        return (feature, self.labels[index])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.count\n",
        "\n",
        "def train(model, device):\n",
        "    data_transforms = transforms.Compose([\n",
        "        ClaheTranform(),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "    torch.manual_seed(1)\n",
        "    train_dataset = PickledTrafficSignsDataset(training_file, transform=data_transforms)\n",
        "    valid_dataset = PickledTrafficSignsDataset(validation_file, transform=data_transforms)\n",
        "    test_dataset = PickledTrafficSignsDataset(testing_file, transform=data_transforms)\n",
        "    class_sample_count = np.bincount(train_dataset.labels)\n",
        "    weights = 1 / np.array([class_sample_count[y] for y in train_dataset.labels])\n",
        "    samp = sampler.WeightedRandomSampler(weights, 43 * 2000)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, sampler=samp)\n",
        "    #train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.005, momentum=0.7)\n",
        "    train_epochs(model, device, train_loader, valid_loader, optimizer)\n",
        "\n",
        "def train_epochs(model, device, train_data_loader, valid_data_loader, optimizer):\n",
        "    \n",
        "    liveloss = PlotLosses()\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    data_loaders = {'train': train_data_loader, 'validation':valid_data_loader}\n",
        "    \n",
        "    for epoch in range(20):\n",
        "        logs = {}\n",
        "        for phase in ['train', 'validation']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "                \n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "            total = 0\n",
        "\n",
        "            for batch_idx, (data, target) in enumerate(data_loaders[phase]):\n",
        "                \n",
        "                if phase == 'train':\n",
        "                    output = model(data.to(device))\n",
        "                    target = target.long().to(device)\n",
        "                    loss = loss_function(output, target)\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                else:\n",
        "                    with torch.no_grad():\n",
        "                        output = model(data.to(device))\n",
        "                        target = target.long().to(device)\n",
        "                        loss = loss_function(output, target)\n",
        "\n",
        "                if batch_idx % 100 == 0:\n",
        "                    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\t{} Loss: {:.6f}'.format(\n",
        "                        epoch, batch_idx * len(data), len(data_loaders[phase].dataset),\n",
        "                        100. * batch_idx / len(data_loaders[phase]), phase, loss.item()))\n",
        "                \n",
        "                pred = torch.argmax(output, dim=1)\n",
        "                running_loss += loss.detach()\n",
        "                running_corrects += torch.sum(pred == target).sum().item() \n",
        "                total += target.size(0)\n",
        "\n",
        "\n",
        "            epoch_loss = running_loss / len(data_loaders[phase].dataset)\n",
        "            epoch_acc = running_corrects / total\n",
        "            \n",
        "            prefix = ''\n",
        "            if phase == 'validation':\n",
        "                prefix = 'val_'\n",
        "\n",
        "            logs[prefix + 'log loss'] = epoch_loss.item()\n",
        "            logs[prefix + 'accuracy'] = epoch_acc#.item()\n",
        "        \n",
        "        liveloss.update(logs)\n",
        "        liveloss.draw()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NML0dL2VEyy4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = TraffficNet(True).to(device)\n",
        "model.share_memory() # gradients are allocated lazily, so they are not shared here\n",
        "train(model, device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4mEYBZmEyy6",
        "colab_type": "text"
      },
      "source": [
        "![](images/traffic_learning.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78BJiQ11Eyy7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_transforms = transforms.Compose([\n",
        "        ClaheTranform(),\n",
        "        transforms.ToTensor()\n",
        "])\n",
        "test_dataset = PickledTrafficSignsDataset(testing_file, transform=data_transforms)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "for (batch_idx, data) in enumerate(test_loader):\n",
        "    with torch.no_grad():\n",
        "        output = model(data[0].to(device))\n",
        "        pred = torch.argmax(output, dim=1)\n",
        "        break\n",
        "        \n",
        "plt.figure(figsize=(20, 20))\n",
        "for i in range(len(pred)):\n",
        "    plt.subplot(11, 6, i+1)\n",
        "    plt.axis('off')\n",
        "    plt.title(signal_names.loc[signal_names['ClassId'] == pred[i].cpu().numpy()].SignName.to_string(index=False))\n",
        "    plt.imshow(np.reshape(data[0][i,...].cpu().numpy(), (-1,32)), cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5iwrbAlEyy9",
        "colab_type": "text"
      },
      "source": [
        "![](images/traffic_sign_test_pred.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BPz9UNnEyy9",
        "colab_type": "text"
      },
      "source": [
        "### Human pose estimation using Deep Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6u8p5jY5VZJ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://posefs1.perception.cs.cmu.edu/OpenPose/models/pose/mpi/pose_iter_160000.caffemodel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhXwaRq3Eyy9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import cv2\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils import data\n",
        "\n",
        "#print(cv2.__version__)\n",
        "\n",
        "proto_file = \"models/pose_deploy_linevec_faster_4_stages.prototxt\"\n",
        "weights_file = \"pose_iter_160000.caffemodel\"\n",
        "n_points = 15\n",
        "body_parts = {\"Head\": 0, \"Neck\": 1, \"RShoulder\": 2, \"RElbow\": 3, \"RWrist\": 4, \n",
        "              \"LShoulder\": 5,  \"LElbow\": 6, \"LWrist\": 7, \"RHip\": 8, \"RKnee\": 9,\n",
        "             \"RAnkle\": 10, \"LHip\": 11, \"LKnee\": 12, \"LAnkle\": 13, \"Chest\": 14, \"Background\": 15}\n",
        "\n",
        "#pose_parts = [[0,1], [1,2], [2,3], [3,4], [1,5], [5,6], [6,7], [1,14], [14,8], [8,9], [9,10], [14,11], [11,12], [12,13] ]\n",
        "pose_parts = [ [\"Head\", \"Neck\"], [\"Neck\", \"RShoulder\"], [\"RShoulder\", \"RElbow\"],\n",
        "             [\"RElbow\", \"RWrist\"], [\"Neck\", \"LShoulder\"], [\"LShoulder\", \"LElbow\"],\n",
        "             [\"LElbow\", \"LWrist\"], [\"Neck\", \"Chest\"], [\"Chest\", \"RHip\"], [\"RHip\", \"RKnee\"],\n",
        "             [\"RKnee\", \"RAnkle\"], [\"Chest\", \"LHip\"], [\"LHip\", \"LKnee\"], [\"LKnee\", \"LAnkle\"] ]\n",
        "\n",
        "\n",
        "image = cv2.imread(\"images/leander.png\")\n",
        "height, width = image.shape[:2]\n",
        "threshold = 0.1\n",
        "\n",
        "net = cv2.dnn.readNetFromCaffe(proto_file, weights_file)\n",
        "blob = cv2.dnn.blobFromImage(image, 1.0 / 255, (368,368), (0, 0, 0), swapRB=False, crop=False)\n",
        "net.setInput(blob)\n",
        "output = net.forward()\n",
        "h, w = output.shape[2:4]\n",
        "print(output.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Hqe-6IjEyy_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=[14,10])\n",
        "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "prob_map = np.zeros((width, height))\n",
        "for i in range(1,5):\n",
        "    pmap = output[0, i, :, :]\n",
        "    prob_map += cv2.resize(pmap, (height, width))\n",
        "plt.imshow(prob_map, alpha=0.6)\n",
        "plt.colorbar()\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUwuVtVTEyzB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image1 = image.copy()\n",
        "\n",
        "# Empty list to store the detected keypoints\n",
        "points = []\n",
        "\n",
        "for i in range(n_points):\n",
        "    # confidence map of corresponding body's part.\n",
        "    prob_map = output[0, i, :, :]\n",
        "\n",
        "    # Find local maxima of the prob_map.\n",
        "    min_val, prob, min_loc, point = cv2.minMaxLoc(prob_map)\n",
        "    \n",
        "    # Scale the point to fit on the original image\n",
        "    x = (width * point[0]) / w\n",
        "    y = (height * point[1]) / h\n",
        "\n",
        "    if prob > threshold : \n",
        "        cv2.circle(image1, (int(x), int(y)), 8, (255, 0, 255), thickness=-1, lineType=cv2.FILLED)\n",
        "        cv2.putText(image1, \"{}\".format(i), (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2, lineType=cv2.LINE_AA)\n",
        "        cv2.circle(image, (int(x), int(y)), 8, (255, 0, 255), thickness=-1, lineType=cv2.FILLED)\n",
        "\n",
        "        # Add the point to the list if the probability is greater than the threshold\n",
        "        points.append((int(x), int(y)))\n",
        "    else :\n",
        "        points.append(None)\n",
        "\n",
        "# Draw Skeleton\n",
        "for pair in pose_parts:\n",
        "    part_from = body_parts[pair[0]]\n",
        "    part_to = body_parts[pair[1]]\n",
        "\n",
        "    if points[part_from] and points[part_to]:\n",
        "        cv2.line(image, points[part_from], points[part_to], (0, 255, 0), 3)\n",
        "\n",
        "plt.figure(figsize=[20,12])\n",
        "plt.subplot(121), plt.imshow(cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)), plt.axis('off'), plt.title('Keypoints', size=20)\n",
        "plt.subplot(122), plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB)), plt.axis('off'), plt.title('Pose', size=20)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FggLbgIHEyzE",
        "colab_type": "text"
      },
      "source": [
        "### Gabor Filter banks for Texture Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKmeMQZwEyzE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#http://slazebni.cs.illinois.edu/research/uiuc_texture_dataset.zip\n",
        "from glob import glob\n",
        "for class_name in glob('images/UIUC_textures/*'):\n",
        "    print(class_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKBhd-7NEyzG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://gogul.dev/software/texture-recognition\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from skimage.io import imread\n",
        "from skimage.color import rgb2gray\n",
        "from skimage.filters import gabor_kernel\n",
        "from scipy import ndimage as ndi\n",
        "\n",
        "# prepare filter bank kernels\n",
        "kernels = []\n",
        "for theta in range(4):\n",
        "    theta = theta / 4. * np.pi\n",
        "    for sigma in (1, 3):\n",
        "        for frequency in (0.05, 0.25):\n",
        "            kernel = np.real(gabor_kernel(frequency, theta=theta, sigma_x=sigma, sigma_y=sigma))\n",
        "            kernels.append(kernel)\n",
        "\n",
        "def power(image, kernel):\n",
        "    # Normalize images for better comparison.\n",
        "    image = (image - image.mean()) / image.std()\n",
        "    return np.sqrt(ndi.convolve(image, np.real(kernel), mode='wrap')**2 +\n",
        "                   ndi.convolve(image, np.imag(kernel), mode='wrap')**2)\n",
        "\n",
        "image_names = ['images/UIUC_textures/woods/T04_01.jpg',\n",
        "               'images/UIUC_textures/stones/T12_01.jpg',\n",
        "               'images/UIUC_textures/bricks/T15_01.jpg',\n",
        "               'images/UIUC_textures/checks/T25_01.jpg',\n",
        "              ]\n",
        "labels = ['woods', 'stones', 'bricks', 'checks']\n",
        "\n",
        "images = []\n",
        "for image_name in image_names:\n",
        "    images.append(rgb2gray(imread(image_name)))\n",
        "\n",
        "# Plot a selection of the filter bank kernels and their responses.\n",
        "results = []\n",
        "kernel_params = []\n",
        "for theta in (0, 1):\n",
        "    theta = theta / 4. * np.pi\n",
        "    for frequency in (0.1, 0.4):\n",
        "        kernel = gabor_kernel(frequency, theta=theta)\n",
        "        params = 'theta=%d,\\nfrequency=%.2f' % (theta * 180 / np.pi, frequency)\n",
        "        kernel_params.append(params)\n",
        "        # Save kernel and the power image for each image\n",
        "        results.append((kernel, [power(img, kernel) for img in images]))\n",
        "\n",
        "fig, axes = plt.subplots(nrows=5, ncols=5, figsize=(20, 20))\n",
        "plt.gray()\n",
        "plt.subplots_adjust(0,0,1,0.95,0.05,0.05)\n",
        "fig.suptitle('Image responses for Gabor filter kernels', fontsize=25)\n",
        "\n",
        "axes[0][0].axis('off')\n",
        "\n",
        "# Plot original images\n",
        "for label, img, ax in zip(labels, images, axes[0][1:]):\n",
        "    ax.imshow(img)\n",
        "    ax.set_title(label, fontsize=15)\n",
        "    ax.axis('off')\n",
        "\n",
        "for label, (kernel, powers), ax_row in zip(kernel_params, results, axes[1:]):\n",
        "    # Plot Gabor kernel\n",
        "    ax = ax_row[0]\n",
        "    ax.imshow(np.real(kernel))\n",
        "    ax.set_ylabel(label, fontsize=15)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "\n",
        "    # Plot Gabor responses with the contrast normalized for each filter\n",
        "    vmin = np.min(powers)\n",
        "    vmax = np.max(powers)\n",
        "    for patch, ax in zip(powers, ax_row[1:]):\n",
        "        ax.imshow(patch, vmin=vmin, vmax=vmax)\n",
        "        ax.axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hu0mF66bEyzJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_feats(image, kernels):\n",
        "    feats = np.zeros((len(kernels), 2), dtype=np.double)\n",
        "    for k, kernel in enumerate(kernels):\n",
        "        filtered = ndi.convolve(image, kernel, mode='wrap')\n",
        "        feats[k, 0] = filtered.mean()\n",
        "        feats[k, 1] = filtered.var()\n",
        "    return feats\n",
        "\n",
        "\n",
        "def match(feats, ref_feats):\n",
        "    min_error = np.inf\n",
        "    min_i = None\n",
        "    for i in range(ref_feats.shape[0]):\n",
        "        error = np.sum((feats - ref_feats[i, :])**2)\n",
        "        if error < min_error:\n",
        "            min_error = error\n",
        "            min_i = i\n",
        "    return min_i\n",
        "\n",
        "# prepare reference features\n",
        "ref_feats = np.zeros((4, len(kernels), 2), dtype=np.double)\n",
        "for i in range(4):\n",
        "    ref_feats[i, :, :] = compute_feats(images[i], kernels)\n",
        "\n",
        "print('Images matched against references using Gabor filter banks:')\n",
        "\n",
        "new_image_names = ['images/UIUC_textures/woods/T04_02.jpg',\n",
        "               'images/UIUC_textures/stones/T12_02.jpg',\n",
        "               'images/UIUC_textures/bricks/T15_02.jpg',\n",
        "               'images/UIUC_textures/checks/T25_02.jpg',\n",
        "              ]\n",
        "\n",
        "plt.figure(figsize=(10,18))\n",
        "plt.subplots_adjust(0,0,1,0.95,0.05,0.05)\n",
        "for i in range(4):\n",
        "    image = rgb2gray(imread(new_image_names[i]))\n",
        "    feats = compute_feats(image, kernels)\n",
        "    mindex = match(feats, ref_feats)\n",
        "    print('original: {}, match result: {} '.format(labels[i], labels[mindex]))\n",
        "    plt.subplot(4,2,2*i+1), plt.imshow(image), plt.axis('off'), plt.title('Original', size=20)\n",
        "    plt.subplot(4,2,2*i+2), plt.imshow(images[mindex]), plt.axis('off'), plt.title('Recognized as ({})'.format(labels[mindex]), size=20)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCXlBZ3kEyzL",
        "colab_type": "text"
      },
      "source": [
        "### Image Classification with Fine Tuning + Transfer learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzYg1orgEyzL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.applications import VGG16\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import models, layers, optimizers\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.preprocessing.image import load_img\n",
        "import matplotlib.pylab as plt\n",
        "import numpy as np\n",
        "\n",
        "train_dir = 'images/flower_photos/train'\n",
        "test_dir = 'images/flower_photos/test'\n",
        "image_size = 224\n",
        "\n",
        "#Load the VGG model\n",
        "vgg_conv = VGG16(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3))\n",
        "\n",
        "# Freeze all the conv layers except the last two\n",
        "for layer in vgg_conv.layers[:-2]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Check the trainable status of the individual layers\n",
        "for layer in vgg_conv.layers:\n",
        "    print(layer, layer.trainable)\n",
        "\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "\n",
        "# Create the model\n",
        "model = models.Sequential()\n",
        "\n",
        "# Add the vgg convolutional base model\n",
        "model.add(vgg_conv)\n",
        "\n",
        "# Add new layers\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(1024, activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "# Show a summary of the model. Check the number of trainable parameters\n",
        "model.summary()\n",
        "\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "    validation_split=0.2) # set validation split\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Change the batchsize according to your system RAM\n",
        "train_batchsize = 100\n",
        "\n",
        "# Data Generator for Training data\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(image_size, image_size),\n",
        "        batch_size=train_batchsize,\n",
        "        class_mode='categorical',\n",
        "        subset='training')\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(image_size, image_size),\n",
        "        batch_size=train_batchsize,\n",
        "        class_mode='categorical',\n",
        "        classes = ['roses', 'sunflowers', 'tulips'],\n",
        "        subset='validation') # set as validation data\n",
        "\n",
        "# Data Generator for Validation data\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "        test_dir,\n",
        "        target_size=(image_size, image_size),\n",
        "        batch_size=1,\n",
        "        class_mode='categorical',\n",
        "        classes = ['roses', 'sunflowers', 'tulips'],\n",
        "        shuffle=False)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=1e-5),\n",
        "              metrics=['acc'])\n",
        "\n",
        "# Train the Model\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch=train_generator.samples/train_generator.batch_size ,\n",
        "      epochs=20,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=validation_generator.samples/validation_generator.batch_size,\n",
        "      verbose=1)\n",
        "\n",
        "# Save the Model\n",
        "model.save('all_freezed.h5')\n",
        "\n",
        "# Plot the accuracy and loss curves\n",
        "acc = history.history['acc']\n",
        "#val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "#val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K06rHDFNEyzN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(20,10))\n",
        "plt.subplots_adjust(left=0, right=1, bottom=0, top=0.95, wspace=0.05, hspace=0)\n",
        "plt.subplot(121)\n",
        "plt.plot(epochs, acc, 'b', label='Training acc')\n",
        "#plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
        "plt.title('Training and validation accuracy', size=20)\n",
        "plt.legend(prop={'size': 10})\n",
        "plt.grid()\n",
        "plt.subplot(122)\n",
        "plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "#plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss', size=20)\n",
        "plt.legend(prop={'size': 10})\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RR034c47EyzP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://github.com/keras-team/keras/issues/3477\n",
        "\n",
        "test_generator.reset()\n",
        "\n",
        "# Get the filenames from the generator\n",
        "fnames = test_generator.filenames\n",
        "\n",
        "# Get the ground truth from generator\n",
        "ground_truth = test_generator.classes\n",
        "\n",
        "# Get the label to class mapping from the generator\n",
        "label2index = test_generator.class_indices\n",
        "\n",
        "# Getting the mapping from class index to class label\n",
        "index2label = dict((v,k) for k,v in label2index.items())\n",
        "\n",
        "# Get the predictions from the model using the generator\n",
        "predictions = model.predict_generator(test_generator, steps=len(fnames))\n",
        "predicted_classes = np.argmax(predictions,axis=-1)\n",
        "predicted_classes = np.array([index2label[k] for k in predicted_classes])\n",
        "ground_truth = np.array([index2label[k] for k in ground_truth])\n",
        "errors = np.where(predicted_classes != ground_truth)[0]\n",
        "print(\"No of errors = {}/{}\".format(len(errors),test_generator.samples))\n",
        "\n",
        "\n",
        "# Show the errors\n",
        "plt.figure(figsize=[20,20])\n",
        "plt.subplots_adjust(left=0, right=1, bottom=0, top=0.95, wspace=0.05, hspace=0)\n",
        "for i in range(16):\n",
        "    pred_label = predicted_classes[errors[i]]\n",
        "    title = 'Original label:{}\\n Prediction: {} confidence: {:.3f}'.format(\n",
        "        ground_truth[errors[i]],\n",
        "        pred_label,\n",
        "        predictions[errors[i]][label2index[pred_label]], size=20)\n",
        "    \n",
        "    original = load_img('{}/{}'.format(test_dir,fnames[errors[i]]))\n",
        "    plt.subplot(4,4,i+1)\n",
        "    plt.axis('off')\n",
        "    plt.title(title, size=15)\n",
        "    plt.imshow(original)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAu_0-8gEyzS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}